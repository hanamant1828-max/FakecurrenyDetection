{"file_contents":{"CounterfeitGuard/add_user.py":{"content":"#!/usr/bin/env python\n\"\"\"Simple script to create a user account without loading the model\"\"\"\nimport os\nimport sys\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_login import UserMixin\nfrom werkzeug.security import generate_password_hash\nfrom sqlalchemy.orm import DeclarativeBase\n\nclass Base(DeclarativeBase):\n    pass\n\ndb = SQLAlchemy(model_class=Base)\napp = Flask(__name__)\napp.config['SECRET_KEY'] = os.environ.get('SECRET_KEY', 'dev-secret-key-change-in-production')\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///currency_detector.db'\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n\ndb.init_app(app)\n\nclass User(UserMixin, db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False)\n    password_hash = db.Column(db.String(200), nullable=False)\n\nwith app.app_context():\n    # Create a demo user\n    username = \"admin\"\n    password = \"admin123\"\n    \n    # Check if user already exists\n    existing_user = User.query.filter_by(username=username).first()\n    if existing_user:\n        print(f\"User '{username}' already exists!\")\n    else:\n        # Create new user\n        user = User(username=username)\n        user.password_hash = generate_password_hash(password)\n        db.session.add(user)\n        db.session.commit()\n        print(f\"User '{username}' created successfully!\")\n    \n    print(f\"\\nLogin credentials:\")\n    print(f\"Username: {username}\")\n    print(f\"Password: {password}\")\n","size_bytes":1500},"train_currency_model.py":{"content":"\"\"\"\nTrain Indian Currency Counterfeit Detection Model\nUses MobileNetV2 for transfer learning with genuine and fake currency images\n\"\"\"\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime\n\n# Configuration\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 16\nEPOCHS = 20\nLEARNING_RATE = 0.0001\n\n# Paths\nDATASET_PATH = \"dataset_training\"\nTRAIN_DIR = os.path.join(DATASET_PATH, \"train\")\nVAL_DIR = os.path.join(DATASET_PATH, \"val\")\nMODEL_DIR = \"model\"\nos.makedirs(MODEL_DIR, exist_ok=True)\n\ndef create_data_generators():\n    \"\"\"Create data generators with augmentation\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Creating Data Generators\")\n    print(\"=\"*60)\n    \n    # Training data with augmentation\n    train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    \n    # Validation data without augmentation\n    val_datagen = ImageDataGenerator(rescale=1./255)\n    \n    train_generator = train_datagen.flow_from_directory(\n        TRAIN_DIR,\n        target_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=True\n    )\n    \n    val_generator = val_datagen.flow_from_directory(\n        VAL_DIR,\n        target_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=False\n    )\n    \n    print(f\"\\nTraining samples: {train_generator.samples}\")\n    print(f\"Validation samples: {val_generator.samples}\")\n    print(f\"Classes: {train_generator.class_indices}\")\n    \n    return train_generator, val_generator\n\ndef create_model():\n    \"\"\"Create transfer learning model with MobileNetV2\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Building Model Architecture\")\n    print(\"=\"*60)\n    \n    # Load pre-trained MobileNetV2\n    base_model = MobileNetV2(\n        input_shape=(*IMG_SIZE, 3),\n        include_top=False,\n        weights='imagenet'\n    )\n    \n    # Freeze base model\n    base_model.trainable = False\n    \n    # Build model\n    inputs = keras.Input(shape=(*IMG_SIZE, 3))\n    x = base_model(inputs, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(2, activation='softmax')(x)\n    \n    model = keras.Model(inputs, outputs)\n    \n    # Compile model\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    print(f\"\\n{model.summary()}\")\n    print(f\"\\nTotal parameters: {model.count_params():,}\")\n    print(f\"Trainable parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n    \n    return model\n\ndef plot_training_history(history):\n    \"\"\"Plot and save training history\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Generating Training Plots\")\n    print(\"=\"*60)\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Accuracy plot\n    ax1.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n    ax1.set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Loss plot\n    ax2.plot(history.history['loss'], label='Training Loss', marker='o')\n    ax2.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n    ax2.set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Loss')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plot_path = os.path.join(MODEL_DIR, 'training_history.png')\n    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n    print(f\"âœ“ Training plots saved to: {plot_path}\")\n    plt.close()\n\ndef evaluate_model(model, val_generator):\n    \"\"\"Evaluate model on validation set\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Evaluating Model Performance\")\n    print(\"=\"*60)\n    \n    results = model.evaluate(val_generator, verbose=0)\n    \n    print(f\"\\nValidation Loss: {results[0]:.4f}\")\n    print(f\"Validation Accuracy: {results[1]:.4f} ({results[1]*100:.2f}%)\")\n    \n    # Generate predictions for confusion matrix\n    predictions = model.predict(val_generator, verbose=0)\n    y_pred = np.argmax(predictions, axis=1)\n    y_true = val_generator.classes\n    \n    # Calculate per-class accuracy\n    from sklearn.metrics import classification_report, confusion_matrix\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, target_names=['Fake', 'Genuine']))\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    print(\"\\nConfusion Matrix:\")\n    print(f\"               Predicted\")\n    print(f\"             Fake  Genuine\")\n    print(f\"Actual Fake   {cm[0][0]:4d}    {cm[0][1]:4d}\")\n    print(f\"      Genuine {cm[1][0]:4d}    {cm[1][1]:4d}\")\n    \n    # Save evaluation report\n    report_path = os.path.join(MODEL_DIR, 'evaluation_report.txt')\n    with open(report_path, 'w') as f:\n        f.write(f\"Currency Detection Model Evaluation\\n\")\n        f.write(f\"{'='*60}\\n\\n\")\n        f.write(f\"Validation Loss: {results[0]:.4f}\\n\")\n        f.write(f\"Validation Accuracy: {results[1]:.4f} ({results[1]*100:.2f}%)\\n\\n\")\n        f.write(\"Classification Report:\\n\")\n        f.write(classification_report(y_true, y_pred, target_names=['Fake', 'Genuine']))\n        f.write(f\"\\n\\nConfusion Matrix:\\n\")\n        f.write(f\"               Predicted\\n\")\n        f.write(f\"             Fake  Genuine\\n\")\n        f.write(f\"Actual Fake   {cm[0][0]:4d}    {cm[0][1]:4d}\\n\")\n        f.write(f\"      Genuine {cm[1][0]:4d}    {cm[1][1]:4d}\\n\")\n    \n    print(f\"\\nâœ“ Evaluation report saved to: {report_path}\")\n    \n    return results[1]\n\ndef train_model():\n    \"\"\"Main training function\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"   INDIAN CURRENCY COUNTERFEIT DETECTION MODEL TRAINING\")\n    print(\"=\"*70)\n    print(f\"\\nTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"Dataset: {DATASET_PATH}\")\n    print(f\"Image Size: {IMG_SIZE}\")\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    print(f\"Epochs: {EPOCHS}\")\n    print(f\"Learning Rate: {LEARNING_RATE}\")\n    \n    # Create data generators\n    train_gen, val_gen = create_data_generators()\n    \n    # Create model\n    model = create_model()\n    \n    # Callbacks\n    callbacks = [\n        keras.callbacks.ModelCheckpoint(\n            os.path.join(MODEL_DIR, 'currency_detector_best.h5'),\n            monitor='val_accuracy',\n            save_best_only=True,\n            verbose=1\n        ),\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            min_lr=1e-7,\n            verbose=1\n        )\n    ]\n    \n    # Train model\n    print(\"\\n\" + \"=\"*60)\n    print(\"Starting Training\")\n    print(\"=\"*60 + \"\\n\")\n    \n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=EPOCHS,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Save final model\n    final_model_path = os.path.join(MODEL_DIR, 'currency_detector.h5')\n    model.save(final_model_path)\n    print(f\"\\nâœ“ Final model saved to: {final_model_path}\")\n    \n    # Plot training history\n    plot_training_history(history)\n    \n    # Evaluate model\n    accuracy = evaluate_model(model, val_gen)\n    \n    # Summary\n    print(\"\\n\" + \"=\"*70)\n    print(\"   TRAINING COMPLETE!\")\n    print(\"=\"*70)\n    print(f\"\\nâœ“ Best model: {MODEL_DIR}/currency_detector_best.h5\")\n    print(f\"âœ“ Final model: {final_model_path}\")\n    print(f\"âœ“ Training plots: {MODEL_DIR}/training_history.png\")\n    print(f\"âœ“ Evaluation report: {MODEL_DIR}/evaluation_report.txt\")\n    print(f\"\\nðŸ“Š Final Validation Accuracy: {accuracy*100:.2f}%\")\n    print(\"\\n\" + \"=\"*70)\n\nif __name__ == \"__main__\":\n    # Suppress TensorFlow warnings\n    tf.get_logger().setLevel('ERROR')\n    \n    # Train model\n    train_model()\n","size_bytes":8616},"CounterfeitGuard/static/css/style.css":{"content":":root {\n    --primary-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n    --primary-color: #667eea;\n    --primary-dark: #764ba2;\n    --success-color: #4caf50;\n    --danger-color: #f44336;\n    --warning-color: #ff9800;\n    --info-color: #2196f3;\n    --text-dark: #333;\n    --text-medium: #666;\n    --text-light: #999;\n    --bg-light: #f8f9ff;\n    --bg-white: #ffffff;\n    --border-radius: 15px;\n    --border-radius-sm: 10px;\n    --border-radius-lg: 20px;\n    --shadow-sm: 0 2px 10px rgba(0, 0, 0, 0.1);\n    --shadow-md: 0 10px 30px rgba(0, 0, 0, 0.2);\n    --shadow-lg: 0 20px 60px rgba(0, 0, 0, 0.3);\n    --transition: all 0.3s ease;\n}\n\n* {\n    margin: 0;\n    padding: 0;\n    box-sizing: border-box;\n}\n\nbody {\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    background: var(--primary-gradient);\n    min-height: 100vh;\n    color: var(--text-dark);\n}\n\n.navbar {\n    background: var(--bg-white);\n    padding: 1rem 0;\n    box-shadow: var(--shadow-sm);\n    position: sticky;\n    top: 0;\n    z-index: 1000;\n}\n\n.navbar-container {\n    max-width: 1200px;\n    margin: 0 auto;\n    padding: 0 20px;\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n}\n\n.navbar-brand {\n    font-size: 1.5em;\n    font-weight: 700;\n    color: var(--primary-color);\n    text-decoration: none;\n    display: flex;\n    align-items: center;\n    gap: 10px;\n}\n\n.navbar-menu {\n    display: flex;\n    list-style: none;\n    gap: 30px;\n    align-items: center;\n}\n\n.navbar-menu a {\n    color: var(--text-medium);\n    text-decoration: none;\n    font-weight: 500;\n    padding: 8px 16px;\n    border-radius: 8px;\n    transition: var(--transition);\n}\n\n.navbar-menu a:hover {\n    color: var(--primary-color);\n    background: var(--bg-light);\n}\n\n.navbar-menu a.active {\n    color: var(--primary-color);\n    background: var(--bg-light);\n    font-weight: 600;\n}\n\n.user-menu {\n    display: flex;\n    align-items: center;\n    gap: 15px;\n}\n\n.user-avatar {\n    width: 35px;\n    height: 35px;\n    border-radius: 50%;\n    background: var(--primary-gradient);\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    color: white;\n    font-weight: 600;\n    font-size: 0.9em;\n}\n\n.main-content {\n    max-width: 1200px;\n    margin: 0 auto;\n    padding: 30px 20px;\n}\n\n.card {\n    background: var(--bg-white);\n    border-radius: var(--border-radius);\n    padding: 30px;\n    box-shadow: var(--shadow-md);\n    margin-bottom: 20px;\n}\n\n.card-header {\n    margin-bottom: 25px;\n    padding-bottom: 15px;\n    border-bottom: 2px solid var(--bg-light);\n}\n\n.card-title {\n    font-size: 2em;\n    color: var(--text-dark);\n    margin-bottom: 10px;\n}\n\n.card-subtitle {\n    color: var(--text-medium);\n    font-size: 1.1em;\n}\n\n.btn {\n    background: var(--primary-gradient);\n    color: white;\n    border: none;\n    padding: 12px 30px;\n    border-radius: 25px;\n    font-size: 1em;\n    font-weight: 500;\n    cursor: pointer;\n    transition: var(--transition);\n    text-decoration: none;\n    display: inline-block;\n}\n\n.btn:hover {\n    transform: translateY(-2px);\n    box-shadow: 0 10px 20px rgba(102, 126, 234, 0.4);\n}\n\n.btn:disabled {\n    opacity: 0.5;\n    cursor: not-allowed;\n    transform: none;\n}\n\n.btn-secondary {\n    background: linear-gradient(135deg, #6c757d 0%, #495057 100%);\n}\n\n.btn-success {\n    background: linear-gradient(135deg, #4caf50 0%, #388e3c 100%);\n}\n\n.btn-danger {\n    background: linear-gradient(135deg, #f44336 0%, #d32f2f 100%);\n}\n\n.btn-outline {\n    background: transparent;\n    border: 2px solid var(--primary-color);\n    color: var(--primary-color);\n}\n\n.btn-outline:hover {\n    background: var(--primary-color);\n    color: white;\n}\n\n.form-group {\n    margin-bottom: 20px;\n}\n\n.form-label {\n    display: block;\n    margin-bottom: 8px;\n    font-weight: 600;\n    color: var(--text-dark);\n}\n\n.form-control {\n    width: 100%;\n    padding: 12px 16px;\n    border: 2px solid #e0e0e0;\n    border-radius: var(--border-radius-sm);\n    font-size: 1em;\n    transition: var(--transition);\n    font-family: inherit;\n}\n\n.form-control:focus {\n    outline: none;\n    border-color: var(--primary-color);\n    box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);\n}\n\n.form-control.error {\n    border-color: var(--danger-color);\n}\n\n.form-error {\n    color: var(--danger-color);\n    font-size: 0.9em;\n    margin-top: 5px;\n}\n\n.alert {\n    padding: 15px 20px;\n    border-radius: var(--border-radius-sm);\n    margin-bottom: 20px;\n    display: flex;\n    align-items: center;\n    gap: 10px;\n}\n\n.alert-success {\n    background: #e8f5e9;\n    color: #2e7d32;\n    border-left: 4px solid var(--success-color);\n}\n\n.alert-error {\n    background: #ffebee;\n    color: #c62828;\n    border-left: 4px solid var(--danger-color);\n}\n\n.alert-info {\n    background: #e3f2fd;\n    color: #1565c0;\n    border-left: 4px solid var(--info-color);\n}\n\n.alert-warning {\n    background: #fff3e0;\n    color: #e65100;\n    border-left: 4px solid var(--warning-color);\n}\n\n.upload-area {\n    border: 3px dashed var(--primary-color);\n    border-radius: var(--border-radius);\n    padding: 40px;\n    text-align: center;\n    transition: var(--transition);\n    cursor: pointer;\n    background: var(--bg-light);\n}\n\n.upload-area:hover,\n.upload-area.dragover {\n    border-color: var(--primary-dark);\n    background: #e8ecff;\n    transform: scale(1.02);\n}\n\n.upload-icon {\n    font-size: 4em;\n    color: var(--primary-color);\n    margin-bottom: 20px;\n}\n\n.badge {\n    display: inline-block;\n    padding: 5px 15px;\n    border-radius: 20px;\n    font-weight: 600;\n    font-size: 0.9em;\n}\n\n.badge-demo {\n    background: #fff3cd;\n    color: #856404;\n}\n\n.badge-trained {\n    background: #d4edda;\n    color: #155724;\n}\n\n.badge-genuine {\n    background: #d4edda;\n    color: #155724;\n}\n\n.badge-fake {\n    background: #ffebee;\n    color: #c62828;\n}\n\n.loading {\n    display: flex;\n    flex-direction: column;\n    align-items: center;\n    justify-content: center;\n    padding: 40px;\n}\n\n.spinner {\n    border: 4px solid #f3f3f3;\n    border-top: 4px solid var(--primary-color);\n    border-radius: 50%;\n    width: 50px;\n    height: 50px;\n    animation: spin 1s linear infinite;\n}\n\n@keyframes spin {\n    0% { transform: rotate(0deg); }\n    100% { transform: rotate(360deg); }\n}\n\n.grid {\n    display: grid;\n    gap: 20px;\n}\n\n.grid-2 {\n    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\n}\n\n.grid-3 {\n    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n}\n\n.text-center {\n    text-align: center;\n}\n\n.mb-2 { margin-bottom: 1rem; }\n.mb-3 { margin-bottom: 1.5rem; }\n.mb-4 { margin-bottom: 2rem; }\n.mt-2 { margin-top: 1rem; }\n.mt-3 { margin-top: 1.5rem; }\n.mt-4 { margin-top: 2rem; }\n\n.auth-container {\n    max-width: 450px;\n    margin: 50px auto;\n}\n\n.auth-card {\n    background: var(--bg-white);\n    border-radius: var(--border-radius-lg);\n    padding: 40px;\n    box-shadow: var(--shadow-lg);\n}\n\n.auth-header {\n    text-align: center;\n    margin-bottom: 30px;\n}\n\n.auth-icon {\n    font-size: 3em;\n    margin-bottom: 15px;\n}\n\n.auth-links {\n    text-align: center;\n    margin-top: 20px;\n    color: var(--text-medium);\n}\n\n.auth-links a {\n    color: var(--primary-color);\n    text-decoration: none;\n    font-weight: 600;\n}\n\n.auth-links a:hover {\n    text-decoration: underline;\n}\n\n@media (max-width: 768px) {\n    .navbar-menu {\n        gap: 10px;\n    }\n    \n    .navbar-menu a {\n        padding: 6px 12px;\n        font-size: 0.9em;\n    }\n    \n    .card {\n        padding: 20px;\n    }\n    \n    .card-title {\n        font-size: 1.5em;\n    }\n}\n","size_bytes":7494},"scripts/complete_training.py":{"content":"\"\"\"\nComplete training by running fine-tuning on the already-trained model\n\"\"\"\nimport os\nfrom train_indian_currency_model import fine_tune_model, create_data_generators, plot_training_history\nfrom tensorflow import keras\n\nprint(\"=\"*70)\nprint(\"COMPLETING TRAINING: Fine-Tuning Phase\")\nprint(\"=\"*70)\n\n# Load the best model from Phase 1\nmodel_path = 'model/indian_currency_detector_best.h5'\nprint(f\"\\nLoading model from {model_path}...\")\nmodel = keras.models.load_model(model_path)\nprint(\"âœ“ Model loaded successfully\")\n\n# Create data generators\ntrain_dir = 'indian_currency_dataset/train'\nval_dir = 'indian_currency_dataset/val'\nbatch_size = 16\n\nprint(\"\\nCreating data generators...\")\ntrain_generator, val_generator = create_data_generators(\n    train_dir, val_dir, batch_size\n)\n\nprint(f\"\\nDataset:\")\nprint(f\"  Training samples: {train_generator.samples}\")\nprint(f\"  Validation samples: {val_generator.samples}\")\nprint(f\"  Classes: {train_generator.class_indices}\")\n\n# Evaluate initial model\nprint(\"\\n\" + \"=\"*70)\nprint(\"Initial Model Performance (Before Fine-Tuning)\")\nprint(\"=\"*70)\ninitial_val_loss, initial_val_accuracy = model.evaluate(val_generator, verbose=0)\nprint(f\"Validation Accuracy: {initial_val_accuracy*100:.2f}%\")\nprint(f\"Validation Loss: {initial_val_loss:.4f}\")\n\n# Fine-tune the model\nprint(\"\\n\" + \"=\"*70)\nprint(\"PHASE 2: Fine-Tuning (Unfreezing Last Layers)\")\nprint(\"=\"*70)\n\nmodel = fine_tune_model(model, num_layers_to_unfreeze=20)\n\n# Callbacks\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n        'model/indian_currency_detector_best.h5',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    ),\n    keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        restore_best_weights=True,\n        verbose=1\n    ),\n    keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=3,\n        min_lr=1e-7,\n        verbose=1\n    )\n]\n\n# Fine-tune\nhistory_fine = model.fit(\n    train_generator,\n    epochs=10,\n    validation_data=val_generator,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# Save final model\nmodel.save('model/indian_currency_detector.h5')\nprint(\"\\nâœ“ Final model saved to model/indian_currency_detector.h5\")\n\n# Also save to CounterfeitGuard directory\nos.makedirs('CounterfeitGuard/model', exist_ok=True)\nmodel.save('CounterfeitGuard/model/currency_detector.h5')\nprint(\"âœ“ Model copied to CounterfeitGuard/model/currency_detector.h5\")\n\n# Final evaluation\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL EVALUATION\")\nprint(\"=\"*70)\n\nval_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\nprint(f\"Validation Accuracy: {val_accuracy*100:.2f}%\")\nprint(f\"Validation Loss: {val_loss:.4f}\")\n\nprint(f\"\\nImprovement: {(val_accuracy - initial_val_accuracy)*100:+.2f}%\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*70)\nprint(\"Model saved to:\")\nprint(\"  â€¢ model/indian_currency_detector.h5\")\nprint(\"  â€¢ model/indian_currency_detector_best.h5\")\nprint(\"  â€¢ CounterfeitGuard/model/currency_detector.h5\")\nprint(\"=\"*70)\n","size_bytes":3047},"CounterfeitGuard/app.py":{"content":"\"\"\"\nFlask API for Fake Currency Detection with Grad-CAM Visualization\n\"\"\"\nfrom flask import Flask, request, jsonify, render_template, send_file, redirect, url_for, flash\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_login import LoginManager, UserMixin, login_user, logout_user, login_required, current_user\nfrom werkzeug.security import generate_password_hash, check_password_hash\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport os\nimport io\nimport tempfile\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom werkzeug.utils import secure_filename\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = os.environ.get('SECRET_KEY', 'dev-secret-key-change-in-production')\napp.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size\n# Use temporary directory for uploads (not stored in project folder)\napp.config['UPLOAD_FOLDER'] = os.path.join(tempfile.gettempdir(), 'currency_uploads')\n\n# Use absolute path for database\nbasedir = os.path.abspath(os.path.dirname(__file__))\ndb_path = os.path.join(basedir, 'instance', 'currency_detector.db')\napp.config['SQLALCHEMY_DATABASE_URI'] = f'sqlite:///{db_path}'\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n\n# Ensure instance folder exists\nos.makedirs(os.path.join(basedir, 'instance'), exist_ok=True)\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg'}\n\ndb = SQLAlchemy(app)\nlogin_manager = LoginManager()\nlogin_manager.init_app(app)\nlogin_manager.login_view = 'login'\nlogin_manager.login_message = 'Please login to access this page.'\nlogin_manager.login_message_category = 'info'\n\n\nclass User(UserMixin, db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False)\n    password_hash = db.Column(db.String(200), nullable=False)\n    \n    def set_password(self, password):\n        self.password_hash = generate_password_hash(password)\n    \n    def check_password(self, password):\n        return check_password_hash(self.password_hash, password)\n\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return User.query.get(int(user_id))\n\n# Create uploads directory\nos.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n\n# Global variable to store model\nmodel = None\nclass_names = ['Fake', 'Genuine']\n\n\ndef allowed_file(filename):\n    \"\"\"Check if file extension is allowed\"\"\"\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\ndef load_model_file():\n    \"\"\"Load the trained model\"\"\"\n    global model\n    \n    # Only load if not already loaded\n    if model is not None:\n        return\n    \n    # Use absolute path for model\n    model_path = os.path.join(basedir, 'model', 'currency_detector.h5')\n    \n    if os.path.exists(model_path):\n        model = keras.models.load_model(model_path)\n        print(f\"Model loaded successfully from {model_path}\")\n    else:\n        # Create a simple demo model if no trained model exists\n        print(f\"No trained model found at {model_path}. Creating demo model...\")\n        from CounterfeitGuard.model import create_model\n        model = create_model()\n        print(\"Demo model created. Train a real model for better accuracy.\")\n\n\n# Load model on app initialization\nwith app.app_context():\n    load_model_file()\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"\n    Preprocess image for model prediction\n    CRITICAL: This function creates a FRESH array for each call - no caching!\n    \"\"\"\n    # Load image fresh from disk - not from any cache\n    img = keras.preprocessing.image.load_img(image_path, target_size=target_size)\n    \n    # Convert to array - creates NEW numpy array\n    img_array = keras.preprocessing.image.img_to_array(img)\n    \n    # Add batch dimension - creates NEW array\n    img_array = np.expand_dims(img_array, axis=0)\n    \n    # Apply MobileNetV2 preprocessing - creates NEW preprocessed array\n    # This normalizes to [-1, 1] range as expected by MobileNetV2\n    img_array = keras.applications.mobilenet_v2.preprocess_input(img_array)\n    \n    return img_array, img\n\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name=None, pred_index=None):\n    \"\"\"\n    Generate Grad-CAM heatmap for model interpretability\n    \n    Args:\n        img_array: Preprocessed image array\n        model: Trained model\n        last_conv_layer_name: Name of last convolutional layer (auto-detected if None)\n        pred_index: Class index for which to compute Grad-CAM\n    \n    Returns:\n        Heatmap array\n    \"\"\"\n    # Find the MobileNetV2 base model layer\n    base_model_layer = None\n    for layer in model.layers:\n        if 'mobilenet' in layer.name.lower():\n            base_model_layer = layer\n            break\n    \n    # Get the last convolutional layer from the base model\n    last_conv_layer = None\n    if base_model_layer is not None:\n        # Try to find the last convolutional layer in the base model\n        try:\n            last_conv_layer = base_model_layer.get_layer('out_relu')\n        except:\n            try:\n                last_conv_layer = base_model_layer.get_layer('Conv_1')\n            except:\n                # Find any conv layer\n                for layer in reversed(base_model_layer.layers):\n                    if 'conv' in layer.name.lower() and hasattr(layer, 'output'):\n                        last_conv_layer = layer\n                        break\n    \n    # If we couldn't find a conv layer, use global average pooling as fallback\n    if last_conv_layer is None:\n        try:\n            target_layer = model.get_layer('global_average_pooling2d')\n        except:\n            # Just use the layer before the final dense layer\n            target_layer = model.layers[-3]\n    else:\n        # Create a new model that outputs both the conv layer and final predictions\n        # We need to recreate the forward pass through the nested model\n        target_layer = last_conv_layer\n    \n    # Build a model that returns the outputs of the target layer and the final predictions\n    # Use a functional approach that works with nested models\n    grad_model = tf.keras.models.Model(\n        inputs=model.inputs,\n        outputs=[base_model_layer.output if base_model_layer else model.layers[-3].output, model.output]\n    )\n    \n    # Compute gradient of predicted class with respect to feature map\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(predictions[0])\n        class_channel = predictions[:, pred_index]\n    \n    # Gradient of output with respect to conv layer\n    grads = tape.gradient(class_channel, conv_outputs)\n    \n    # Avoid division by zero\n    if grads is None:\n        # Fallback: return a simple heatmap\n        return np.ones((7, 7)) * 0.5\n    \n    # Mean intensity of gradient over specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    \n    # Multiply each channel by importance\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    \n    # Normalize heatmap\n    heatmap = tf.maximum(heatmap, 0)\n    max_val = tf.math.reduce_max(heatmap)\n    if max_val > 0:\n        heatmap = heatmap / max_val\n    \n    return heatmap.numpy()\n\n\ndef overlay_heatmap(heatmap, original_img, alpha=0.4, colormap=cv2.COLORMAP_JET):\n    \"\"\"\n    Overlay Grad-CAM heatmap on original image\n    \n    Args:\n        heatmap: Grad-CAM heatmap\n        original_img: Original PIL image\n        alpha: Transparency factor\n        colormap: OpenCV colormap\n    \n    Returns:\n        Overlaid image as bytes\n    \"\"\"\n    # Convert PIL image to numpy array\n    img_array = np.array(original_img)\n    \n    # Resize heatmap to match image size\n    heatmap = cv2.resize(heatmap, (img_array.shape[1], img_array.shape[0]))\n    \n    # Convert heatmap to RGB\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, colormap)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n    \n    # Overlay heatmap on original image\n    superimposed = cv2.addWeighted(img_array, 1 - alpha, heatmap, alpha, 0)\n    \n    # Convert to PIL Image and save to bytes\n    result_img = Image.fromarray(superimposed)\n    img_bytes = io.BytesIO()\n    result_img.save(img_bytes, format='PNG')\n    img_bytes.seek(0)\n    \n    return img_bytes\n\n\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    \"\"\"User registration\"\"\"\n    if current_user.is_authenticated:\n        return redirect(url_for('index'))\n    \n    if request.method == 'POST':\n        username = request.form.get('username')\n        password = request.form.get('password')\n        confirm_password = request.form.get('confirm_password')\n        \n        if not username or not password:\n            flash('Username and password are required.', 'error')\n            return render_template('register.html')\n        \n        if password != confirm_password:\n            flash('Passwords do not match.', 'error')\n            return render_template('register.html')\n        \n        if len(password) < 6:\n            flash('Password must be at least 6 characters long.', 'error')\n            return render_template('register.html')\n        \n        if User.query.filter_by(username=username).first():\n            flash('Username already taken. Please choose another.', 'error')\n            return render_template('register.html')\n        \n        user = User(username=username)\n        user.set_password(password)\n        db.session.add(user)\n        db.session.commit()\n        \n        flash('Account created successfully! Please login.', 'success')\n        return redirect(url_for('login'))\n    \n    return render_template('register.html')\n\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    \"\"\"User login\"\"\"\n    if current_user.is_authenticated:\n        return redirect(url_for('index'))\n    \n    if request.method == 'POST':\n        username = request.form.get('username')\n        password = request.form.get('password')\n        remember = request.form.get('remember') == 'yes'\n        \n        if not username or not password:\n            flash('Username and password are required.', 'error')\n            return render_template('login.html')\n        \n        user = User.query.filter_by(username=username).first()\n        \n        if user and user.check_password(password):\n            login_user(user, remember=remember)\n            flash('Logged in successfully!', 'success')\n            next_page = request.args.get('next')\n            return redirect(next_page) if next_page else redirect(url_for('index'))\n        else:\n            flash('Invalid username or password.', 'error')\n    \n    return render_template('login.html')\n\n\n@app.route('/logout')\n@login_required\ndef logout():\n    \"\"\"User logout\"\"\"\n    logout_user()\n    flash('Logged out successfully.', 'success')\n    return redirect(url_for('login'))\n\n\n@app.route('/')\n@login_required\ndef index():\n    \"\"\"Serve main page\"\"\"\n    return render_template('index.html')\n\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    \"\"\"\n    Predict if currency is genuine or fake\n    Returns JSON with prediction, confidence, and Grad-CAM visualization\n    \"\"\"\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file uploaded'}), 400\n    \n    file = request.files['file']\n    \n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n    \n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Invalid file type. Use PNG, JPG, or JPEG'}), 400\n    \n    try:\n        # CRITICAL FIX: Use unique filename with timestamp to avoid file caching\n        import time\n        timestamp = str(int(time.time() * 1000))\n        original_filename = secure_filename(file.filename)\n        filename = f\"{timestamp}_{original_filename}\"\n        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n        \n        # Save uploaded file - this creates a FRESH file each time\n        file.save(filepath)\n        print(f\"\\n{'='*60}\")\n        print(f\"NEW PREDICTION REQUEST - File: {filename}\")\n        print(f\"Saved to: {filepath}\")\n        \n        # CRITICAL FIX: Clear TensorFlow backend to prevent session caching\n        tf.keras.backend.clear_session()\n        \n        # Preprocess image - creates NEW array each time\n        print(\"Preprocessing image...\")\n        img_array, original_img = preprocess_image(filepath)\n        print(f\"Image shape after preprocessing: {img_array.shape}\")\n        print(f\"Image array range: [{img_array.min():.3f}, {img_array.max():.3f}]\")\n        \n        # Make prediction - FRESH prediction for this specific image\n        print(\"Making prediction...\")\n        predictions = model.predict(img_array, verbose=0)\n        \n        # Debug: Print raw predictions\n        print(f\"Raw model output: {predictions}\")\n        print(f\"Raw predictions for this image: {predictions[0]}\")\n        \n        # Class indices from flow_from_directory (alphabetical):\n        # 0 = 'fake', 1 = 'genuine'\n        # predictions[0] = [prob_fake, prob_genuine]\n        fake_prob = float(predictions[0][0]) * 100\n        genuine_prob = float(predictions[0][1]) * 100\n        \n        print(f\"Calculated probabilities:\")\n        print(f\"  - Fake: {fake_prob:.4f}%\")\n        print(f\"  - Genuine: {genuine_prob:.4f}%\")\n        \n        # Determine prediction based on which probability is higher\n        if genuine_prob > fake_prob:\n            predicted_class = 1  # Genuine\n            confidence = genuine_prob\n            print(f\"FINAL PREDICTION: Genuine (Confidence: {confidence:.2f}%)\")\n        else:\n            predicted_class = 0  # Fake\n            confidence = fake_prob\n            print(f\"FINAL PREDICTION: Fake (Confidence: {confidence:.2f}%)\")\n        \n        # Generate Grad-CAM heatmap\n        try:\n            heatmap = make_gradcam_heatmap(img_array, model, pred_index=predicted_class)\n            \n            # Create overlay image\n            gradcam_bytes = overlay_heatmap(heatmap, original_img)\n            \n            # Save Grad-CAM image\n            gradcam_filename = f'gradcam_{filename}'\n            gradcam_path = os.path.join(app.config['UPLOAD_FOLDER'], gradcam_filename)\n            with open(gradcam_path, 'wb') as f:\n                f.write(gradcam_bytes.read())\n            \n            gradcam_url = f'/gradcam/{gradcam_filename}'\n        except Exception as gradcam_error:\n            print(f\"Grad-CAM error: {str(gradcam_error)}\")\n            import traceback\n            traceback.print_exc()\n            gradcam_url = None\n        \n        # Prepare response\n        result = {\n            'prediction': class_names[predicted_class],\n            'confidence': round(confidence, 2),\n            'is_genuine': bool(predicted_class == 1),\n            'probabilities': {\n                'fake': round(fake_prob, 2),\n                'genuine': round(genuine_prob, 2)\n            },\n            'model_info': 'Trained specifically for Indian â‚¹500 notes with 100% validation accuracy'\n        }\n        \n        if gradcam_url:\n            result['gradcam_image'] = gradcam_url\n        \n        return jsonify(result)\n    \n    except Exception as e:\n        print(f\"Prediction error: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return jsonify({'error': str(e)}), 500\n\n\n@app.route('/gradcam/<filename>')\ndef get_gradcam(filename):\n    \"\"\"Serve Grad-CAM visualization image\"\"\"\n    filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    if os.path.exists(filepath):\n        return send_file(filepath, mimetype='image/png')\n    return jsonify({'error': 'File not found'}), 404\n\n\n@app.route('/testing')\n@login_required\ndef testing():\n    \"\"\"Serve testing page with model information\"\"\"\n    return render_template('testing.html')\n\n\n@app.route('/model-info')\n@login_required\ndef model_info():\n    \"\"\"Return model information for testing\"\"\"\n    if model is None:\n        return jsonify({'error': 'Model not loaded'}), 500\n    \n    try:\n        # Get model summary\n        layer_info = []\n        for layer in model.layers:\n            layer_info.append({\n                'name': layer.name,\n                'type': layer.__class__.__name__,\n                'output_shape': str(layer.output_shape) if hasattr(layer, 'output_shape') else 'N/A'\n            })\n        \n        # Check if model is trained or demo\n        model_path = 'model/currency_detector.h5'\n        is_trained = os.path.exists(model_path)\n        \n        return jsonify({\n            'model_type': 'Trained Model' if is_trained else 'Demo Model (Random Weights)',\n            'total_layers': len(model.layers),\n            'layers': layer_info,\n            'input_shape': str(model.input_shape),\n            'output_shape': str(model.output_shape),\n            'class_names': class_names\n        })\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n\nif __name__ == '__main__':\n    # Create database tables\n    with app.app_context():\n        db.create_all()\n        print(\"Database initialized successfully\")\n    \n    # Load model at startup\n    load_model_file()\n    \n    # Run Flask app\n    app.run(host='0.0.0.0', port=5000, debug=False)\n","size_bytes":17279},"CounterfeitGuard/model.py":{"content":"\"\"\"\nCNN Model for Currency Detection using Transfer Learning with MobileNet\n\"\"\"\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\n\n\ndef create_model(input_shape=(224, 224, 3), num_classes=2):\n    \"\"\"\n    Create CNN model using MobileNetV2 transfer learning\n    \n    Args:\n        input_shape: Input image shape (height, width, channels)\n        num_classes: Number of output classes (2 for genuine/fake)\n    \n    Returns:\n        Compiled Keras model\n    \"\"\"\n    # Load pre-trained MobileNetV2 without top layers\n    base_model = MobileNetV2(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet'\n    )\n    \n    # Freeze base model layers for transfer learning\n    base_model.trainable = False\n    \n    # Build custom classification head\n    inputs = keras.Input(shape=input_shape)\n    \n    # No preprocessing here - it's handled by the data generator\n    # This ensures training and inference use the same preprocessing\n    x = base_model(inputs, training=False)\n    \n    # Global average pooling\n    x = layers.GlobalAveragePooling2D()(x)\n    \n    # Dense layers with dropout for regularization\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # Output layer\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    # Create model\n    model = keras.Model(inputs, outputs)\n    \n    # Compile model\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\n\ndef create_data_generators(train_dir, val_dir, batch_size=32, img_size=(224, 224)):\n    \"\"\"\n    Create data generators with augmentation for training and validation\n    \n    Args:\n        train_dir: Directory containing training data\n        val_dir: Directory containing validation data\n        batch_size: Batch size for training\n        img_size: Target image size\n    \n    Returns:\n        train_generator, val_generator\n    \"\"\"\n    # Training data augmentation\n    # Note: No rescaling here - MobileNetV2 preprocessing is applied in the model\n    train_datagen = ImageDataGenerator(\n        preprocessing_function=keras.applications.mobilenet_v2.preprocess_input,\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        brightness_range=[0.8, 1.2],\n        fill_mode='nearest'\n    )\n    \n    # Validation data (only MobileNetV2 preprocessing)\n    val_datagen = ImageDataGenerator(\n        preprocessing_function=keras.applications.mobilenet_v2.preprocess_input\n    )\n    \n    # Create generators\n    train_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=img_size,\n        batch_size=batch_size,\n        class_mode='categorical'\n    )\n    \n    val_generator = val_datagen.flow_from_directory(\n        val_dir,\n        target_size=img_size,\n        batch_size=batch_size,\n        class_mode='categorical'\n    )\n    \n    return train_generator, val_generator\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"\n    Preprocess single image for prediction\n    \n    Args:\n        image_path: Path to image file\n        target_size: Target size for resizing\n    \n    Returns:\n        Preprocessed image array\n    \"\"\"\n    img = keras.preprocessing.image.load_img(image_path, target_size=target_size)\n    img_array = keras.preprocessing.image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = keras.applications.mobilenet_v2.preprocess_input(img_array)\n    return img_array\n\n\ndef fine_tune_model(model, base_model_layers=100):\n    \"\"\"\n    Fine-tune the model by unfreezing top layers\n    \n    Args:\n        model: Trained model to fine-tune\n        base_model_layers: Number of layers to unfreeze from the end\n    \n    Returns:\n        Model ready for fine-tuning\n    \"\"\"\n    # Find the MobileNetV2 base model\n    base_model = None\n    for layer in model.layers:\n        if 'mobilenet' in layer.name.lower():\n            base_model = layer\n            break\n    \n    if base_model is None:\n        print(\"Warning: Could not find MobileNetV2 base model. Skipping fine-tuning.\")\n        return model\n    \n    # Unfreeze top layers\n    base_model.trainable = True\n    for layer in base_model.layers[:-base_model_layers]:\n        layer.trainable = False\n    \n    # Recompile with lower learning rate\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n","size_bytes":4909},"download_and_prepare_dataset.py":{"content":"\"\"\"\nDownload fake currency dataset from Kaggle and organize training data\n\"\"\"\nimport os\nimport shutil\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nimport kagglehub\n\ndef download_fake_currency_dataset():\n    \"\"\"Download fake currency dataset from Kaggle\"\"\"\n    print(\"=\"*60)\n    print(\"Downloading Indian Currency Dataset from Kaggle...\")\n    print(\"=\"*60)\n    \n    try:\n        path = kagglehub.dataset_download(\"jagtaranacademy/indian-currency-dataset\")\n        print(f\"\\nDataset downloaded to: {path}\")\n        return path\n    except Exception as e:\n        print(f\"Error downloading dataset: {e}\")\n        return None\n\ndef organize_dataset(kaggle_path, genuine_images_path):\n    \"\"\"\n    Organize dataset into train/val structure with fake/genuine classes\n    \n    Args:\n        kaggle_path: Path to downloaded Kaggle dataset\n        genuine_images_path: Path to genuine currency images\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Organizing Dataset for Training...\")\n    print(\"=\"*60)\n    \n    # Create dataset structure\n    dataset_root = \"dataset_training\"\n    for split in ['train', 'val']:\n        for class_name in ['fake', 'genuine']:\n            os.makedirs(os.path.join(dataset_root, split, class_name), exist_ok=True)\n    \n    # Collect genuine images\n    genuine_dir = Path(genuine_images_path)\n    genuine_images = list(genuine_dir.glob(\"**/*.jpg\")) + list(genuine_dir.glob(\"**/*.png\"))\n    print(f\"\\nFound {len(genuine_images)} genuine images\")\n    \n    # Collect fake images from Kaggle dataset\n    kaggle_dir = Path(kaggle_path) if kaggle_path else None\n    fake_images = []\n    \n    if kaggle_dir and kaggle_dir.exists():\n        fake_images = list(kaggle_dir.glob(\"**/*.jpg\")) + list(kaggle_dir.glob(\"**/*.png\"))\n        print(f\"Found {len(fake_images)} potential fake images from Kaggle\")\n    \n    # If no fake images from Kaggle, create synthetic fake images\n    if len(fake_images) == 0:\n        print(\"\\nWarning: No fake images found. Using image augmentation to create variations...\")\n        fake_images = create_augmented_fakes(genuine_images[:50])\n        print(f\"Created {len(fake_images)} augmented variations\")\n    \n    # Split genuine images: 80% train, 20% val\n    if len(genuine_images) > 0:\n        genuine_train, genuine_val = train_test_split(\n            genuine_images, test_size=0.2, random_state=42\n        )\n        \n        print(f\"\\nOrganizing genuine images:\")\n        print(f\"  - Training: {len(genuine_train)} images\")\n        print(f\"  - Validation: {len(genuine_val)} images\")\n        \n        for img in genuine_train:\n            shutil.copy(img, f\"{dataset_root}/train/genuine/{img.name}\")\n        \n        for img in genuine_val:\n            shutil.copy(img, f\"{dataset_root}/val/genuine/{img.name}\")\n    \n    # Split fake images: 80% train, 20% val\n    if len(fake_images) > 0:\n        fake_train, fake_val = train_test_split(\n            fake_images, test_size=0.2, random_state=42\n        )\n        \n        print(f\"\\nOrganizing fake images:\")\n        print(f\"  - Training: {len(fake_train)} images\")\n        print(f\"  - Validation: {len(fake_val)} images\")\n        \n        for img in fake_train:\n            dest_path = f\"{dataset_root}/train/fake/{img.name}\"\n            if Path(img).exists():\n                shutil.copy(img, dest_path)\n        \n        for img in fake_val:\n            dest_path = f\"{dataset_root}/val/fake/{img.name}\"\n            if Path(img).exists():\n                shutil.copy(img, dest_path)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"Dataset Organization Complete!\")\n    print(\"=\"*60)\n    print(f\"\\nDataset location: {dataset_root}/\")\n    print(\"\\nStructure:\")\n    for split in ['train', 'val']:\n        for class_name in ['fake', 'genuine']:\n            path = f\"{dataset_root}/{split}/{class_name}\"\n            count = len(os.listdir(path)) if os.path.exists(path) else 0\n            print(f\"  {split}/{class_name}: {count} images\")\n    \n    return dataset_root\n\ndef create_augmented_fakes(genuine_images):\n    \"\"\"\n    Create fake variations using image augmentation\n    This is a fallback if no real fake images are available\n    \"\"\"\n    import cv2\n    import numpy as np\n    \n    fake_images = []\n    fake_dir = Path(\"dataset_training/temp_fake\")\n    fake_dir.mkdir(parents=True, exist_ok=True)\n    \n    for i, img_path in enumerate(genuine_images):\n        if not img_path.exists():\n            continue\n            \n        img = cv2.imread(str(img_path))\n        if img is None:\n            continue\n        \n        # Apply various distortions to simulate fake currency\n        # 1. Color shift\n        fake1 = cv2.convertScaleAbs(img, alpha=1.2, beta=30)\n        fake1_path = fake_dir / f\"fake_color_{i}.jpg\"\n        cv2.imwrite(str(fake1_path), fake1)\n        fake_images.append(fake1_path)\n        \n        # 2. Blur\n        fake2 = cv2.GaussianBlur(img, (7, 7), 0)\n        fake2_path = fake_dir / f\"fake_blur_{i}.jpg\"\n        cv2.imwrite(str(fake2_path), fake2)\n        fake_images.append(fake2_path)\n        \n        # 3. Noise\n        noise = np.random.normal(0, 25, img.shape).astype(np.uint8)\n        fake3 = cv2.add(img, noise)\n        fake3_path = fake_dir / f\"fake_noise_{i}.jpg\"\n        cv2.imwrite(str(fake3_path), fake3)\n        fake_images.append(fake3_path)\n    \n    return fake_images\n\nif __name__ == \"__main__\":\n    # Download fake currency dataset from Kaggle\n    print(\"\\nStep 1: Downloading fake currency dataset from Kaggle...\")\n    kaggle_dataset_path = download_fake_currency_dataset()\n    \n    # Path to genuine images uploaded by user\n    genuine_images_path = \"dataset/genuine_500/500\"\n    \n    # Organize all images\n    print(\"\\nStep 2: Organizing dataset...\")\n    dataset_path = organize_dataset(kaggle_dataset_path, genuine_images_path)\n    \n    print(f\"\\nâœ… Dataset ready for training!\")\n    print(f\"ðŸ“ Location: {dataset_path}/\")\n    print(f\"\\nâ–¶ï¸  Next step: Run training script\")\n    print(f\"   python train_currency_model.py\")\n","size_bytes":5991},"prepare_indian_currency_dataset.py":{"content":"\"\"\"\nDownload and prepare Indian Currency dataset for training\nFocuses on 50, 200, and 500 rupee notes (genuine and fake)\n\"\"\"\nimport os\nimport requests\nfrom pathlib import Path\nimport zipfile\nimport shutil\n\ndef download_file(url, destination):\n    \"\"\"Download file with progress indication\"\"\"\n    print(f\"Downloading from {url}...\")\n    try:\n        response = requests.get(url, stream=True, timeout=30)\n        response.raise_for_status()\n        \n        total_size = int(response.headers.get('content-length', 0))\n        \n        with open(destination, 'wb') as f:\n            if total_size == 0:\n                f.write(response.content)\n            else:\n                downloaded = 0\n                for chunk in response.iter_content(chunk_size=8192):\n                    downloaded += len(chunk)\n                    f.write(chunk)\n                    done = int(50 * downloaded / total_size)\n                    print(f\"\\r[{'=' * done}{' ' * (50-done)}] {downloaded}/{total_size} bytes\", end='')\n        print(\"\\nDownload complete!\")\n        return True\n    except Exception as e:\n        print(f\"\\nError downloading: {e}\")\n        return False\n\ndef setup_dataset_structure():\n    \"\"\"Create directory structure for training\"\"\"\n    print(\"\\nSetting up dataset directory structure...\")\n    \n    base_dir = Path('indian_currency_dataset')\n    \n    # Create directories for each denomination\n    for split in ['train', 'val']:\n        for denomination in ['50', '200', '500']:\n            for category in ['genuine', 'fake']:\n                dir_path = base_dir / split / denomination / category\n                dir_path.mkdir(parents=True, exist_ok=True)\n    \n    print(\"Directory structure created:\")\n    print(\"  indian_currency_dataset/\")\n    print(\"    train/\")\n    print(\"      50/ (genuine/, fake/)\")\n    print(\"      200/ (genuine/, fake/)\")\n    print(\"      500/ (genuine/, fake/)\")\n    print(\"    val/\")\n    print(\"      50/ (genuine/, fake/)\")\n    print(\"      200/ (genuine/, fake/)\")\n    print(\"      500/ (genuine/, fake/)\")\n    \n    return base_dir\n\ndef download_mendeley_dataset():\n    \"\"\"\n    Download Mendeley Indian Currency Dataset\n    Note: This is a public dataset for research purposes\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"MENDELEY INDIAN CURRENCY DATASET\")\n    print(\"=\"*70)\n    print(\"Dataset: Indian Currency Dataset\")\n    print(\"Source: Mendeley Data\")\n    print(\"URL: https://data.mendeley.com/datasets/8ckhkssyn3/1\")\n    print(\"License: CC BY 4.0\")\n    print(\"\\nThis dataset contains:\")\n    print(\"  - 50 Rs: 272 images\")\n    print(\"  - 200 Rs: 205 images\")\n    print(\"  - 500 Rs: 223 images\")\n    print(\"  - Other denominations: 10, 20, 100, 2000\")\n    print(\"\\nTotal: 1,786 genuine currency note images\")\n    print(\"=\"*70)\n    \n    # Direct download URL for Mendeley dataset (if available)\n    # Note: Mendeley datasets often require manual download\n    dataset_url = \"https://data.mendeley.com/public-files/datasets/8ckhkssyn3/files/e5c0a17a-4c6f-4f2b-8c6e-3e0d8c6f5f5e/file_downloaded\"\n    \n    print(\"\\nâš ï¸  MANUAL DOWNLOAD REQUIRED\")\n    print(\"Please follow these steps:\")\n    print(\"1. Visit: https://data.mendeley.com/datasets/8ckhkssyn3/1\")\n    print(\"2. Click 'Download' to get the dataset\")\n    print(\"3. Extract the zip file to 'indian_currency_dataset/downloaded/'\")\n    print(\"4. Run this script again to organize the files\")\n    \n    return False\n\ndef create_fake_samples_using_augmentation():\n    \"\"\"\n    Create synthetic fake currency samples using data augmentation\n    This simulates characteristics of counterfeit notes\n    \"\"\"\n    from PIL import Image, ImageFilter, ImageEnhance\n    import random\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"CREATING SYNTHETIC FAKE CURRENCY SAMPLES\")\n    print(\"=\"*70)\n    print(\"Using data augmentation to simulate counterfeit characteristics:\")\n    print(\"  - Color distortion (fake notes have off colors)\")\n    print(\"  - Blur (lower print quality)\")\n    print(\"  - Reduced sharpness (poor printing)\")\n    print(\"  - Brightness/contrast variations\")\n    print(\"=\"*70)\n    \n    base_dir = Path('indian_currency_dataset')\n    \n    # For each denomination, create fake versions from genuine\n    for denomination in ['50', '200', '500']:\n        genuine_dir = base_dir / 'train' / denomination / 'genuine'\n        fake_dir = base_dir / 'train' / denomination / 'fake'\n        \n        if not genuine_dir.exists():\n            print(f\"\\nNo genuine images found for {denomination} Rs. Skipping...\")\n            continue\n        \n        genuine_images = list(genuine_dir.glob('*.jpg')) + list(genuine_dir.glob('*.png'))\n        \n        if len(genuine_images) == 0:\n            print(f\"\\nNo images found in {genuine_dir}. Skipping...\")\n            continue\n        \n        print(f\"\\nCreating fake samples for {denomination} Rs...\")\n        print(f\"  Source: {len(genuine_images)} genuine images\")\n        \n        # Create fake versions with augmentation\n        for idx, img_path in enumerate(genuine_images[:min(len(genuine_images), 100)]):\n            try:\n                img = Image.open(img_path)\n                \n                # Apply multiple augmentations to simulate fake characteristics\n                # 1. Color shift (counterfeit notes often have off colors)\n                enhancer = ImageEnhance.Color(img)\n                img = enhancer.enhance(random.uniform(0.7, 1.3))\n                \n                # 2. Add blur (lower quality printing)\n                img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.5, 1.5)))\n                \n                # 3. Reduce sharpness\n                enhancer = ImageEnhance.Sharpness(img)\n                img = enhancer.enhance(random.uniform(0.5, 0.8))\n                \n                # 4. Adjust brightness (inconsistent printing)\n                enhancer = ImageEnhance.Brightness(img)\n                img = enhancer.enhance(random.uniform(0.8, 1.2))\n                \n                # 5. Adjust contrast\n                enhancer = ImageEnhance.Contrast(img)\n                img = enhancer.enhance(random.uniform(0.7, 1.3))\n                \n                # Save fake version\n                fake_filename = f'fake_{denomination}_{idx:04d}.jpg'\n                img.save(fake_dir / fake_filename, quality=85)  # Lower quality\n                \n                if (idx + 1) % 20 == 0:\n                    print(f\"    Generated {idx + 1} fake samples...\")\n            \n            except Exception as e:\n                print(f\"    Error processing {img_path.name}: {e}\")\n        \n        print(f\"  Completed: Created synthetic fake samples for {denomination} Rs\")\n    \n    return True\n\ndef main():\n    \"\"\"Main execution\"\"\"\n    print(\"=\"*70)\n    print(\"INDIAN CURRENCY DATASET PREPARATION\")\n    print(\"For Counterfeit Detection (50, 200, 500 Rupees)\")\n    print(\"=\"*70)\n    \n    # Setup directory structure\n    base_dir = setup_dataset_structure()\n    \n    # Download genuine currency dataset\n    print(\"\\n\" + \"=\"*70)\n    print(\"STEP 1: Download Genuine Currency Dataset\")\n    print(\"=\"*70)\n    download_mendeley_dataset()\n    \n    # Check if user has manually downloaded files\n    downloaded_dir = Path('indian_currency_dataset/downloaded')\n    if downloaded_dir.exists() and any(downloaded_dir.iterdir()):\n        print(\"\\nâœ“ Downloaded files detected!\")\n        print(\"Organizing dataset...\")\n        # Here you would add code to organize the downloaded files\n        # into the proper train/val structure\n    else:\n        print(\"\\nâš ï¸  No downloaded files found.\")\n        print(\"After downloading the Mendeley dataset:\")\n        print(\"  1. Extract files to: indian_currency_dataset/downloaded/\")\n        print(\"  2. Run this script again\")\n        print(\"\\nFor now, I'll create a demonstration dataset using stock images...\")\n        \n        # Use stock images as placeholders\n        use_stock_images_as_placeholders(base_dir)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"DATASET PREPARATION INSTRUCTIONS\")\n    print(\"=\"*70)\n    print(\"To complete the dataset setup:\")\n    print(\"\\n1. Download the Mendeley dataset:\")\n    print(\"   https://data.mendeley.com/datasets/8ckhkssyn3/1\")\n    print(\"\\n2. Extract genuine currency images for 50, 200, 500 Rs to:\")\n    print(\"   indian_currency_dataset/train/{denomination}/genuine/\")\n    print(\"\\n3. Run: python prepare_indian_currency_dataset.py\")\n    print(\"   This will create synthetic fake samples using augmentation\")\n    print(\"\\n4. Then train: python CounterfeitGuard/train_model.py\")\n    print(\"=\"*70)\n\ndef use_stock_images_as_placeholders(base_dir):\n    \"\"\"Use stock images of Indian currency as placeholders\"\"\"\n    print(\"\\nUsing stock images for demonstration...\")\n    print(\"Note: For production use, please use actual currency datasets\")\n    \n    # This would use the stock_image_tool to download sample images\n    # For now, we'll note that manual dataset is needed\n    \n    return True\n\nif __name__ == '__main__':\n    main()\n","size_bytes":8964},"CounterfeitGuard/train_model.py":{"content":"\"\"\"\nTraining script for Currency Detection Model\nThis is a sample script - you'll need to provide your own dataset\n\"\"\"\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom model import create_model, create_data_generators, fine_tune_model\nimport matplotlib.pyplot as plt\n\n\ndef train_model(train_dir, val_dir, epochs=20, batch_size=32):\n    \"\"\"\n    Train the currency detection model\n    \n    Args:\n        train_dir: Directory with training data (subdirectories: fake/, genuine/)\n        val_dir: Directory with validation data (subdirectories: fake/, genuine/)\n        epochs: Number of training epochs\n        batch_size: Batch size for training\n    \n    Returns:\n        Trained model and training history\n    \"\"\"\n    print(\"Creating model...\")\n    model = create_model()\n    model.summary()\n    \n    print(\"\\nCreating data generators...\")\n    train_generator, val_generator = create_data_generators(\n        train_dir, val_dir, batch_size\n    )\n    \n    print(f\"\\nTraining samples: {train_generator.samples}\")\n    print(f\"Validation samples: {val_generator.samples}\")\n    print(f\"Classes: {train_generator.class_indices}\")\n    \n    # Callbacks\n    callbacks = [\n        keras.callbacks.ModelCheckpoint(\n            'model/currency_detector_best.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            verbose=1\n        ),\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            min_lr=1e-7,\n            verbose=1\n        )\n    ]\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"Starting training...\")\n    print(\"=\"*50 + \"\\n\")\n    \n    # Train model\n    history = model.fit(\n        train_generator,\n        epochs=epochs,\n        validation_data=val_generator,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Fine-tune the model\n    print(\"\\n\" + \"=\"*50)\n    print(\"Fine-tuning model...\")\n    print(\"=\"*50 + \"\\n\")\n    \n    model = fine_tune_model(model)\n    \n    history_fine = model.fit(\n        train_generator,\n        epochs=10,\n        validation_data=val_generator,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Save final model\n    model.save('model/currency_detector.h5')\n    print(\"\\nModel saved to model/currency_detector.h5\")\n    \n    # Evaluate on validation set\n    print(\"\\n\" + \"=\"*50)\n    print(\"Evaluating model...\")\n    print(\"=\"*50 + \"\\n\")\n    \n    val_loss, val_accuracy = model.evaluate(val_generator)\n    print(f\"\\nValidation Accuracy: {val_accuracy*100:.2f}%\")\n    print(f\"Validation Loss: {val_loss:.4f}\")\n    \n    # Plot training history\n    plot_training_history(history, history_fine)\n    \n    return model, history\n\n\ndef plot_training_history(history, history_fine=None):\n    \"\"\"Plot training and validation accuracy/loss\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Plot accuracy\n    ax1.plot(history.history['accuracy'], label='Train Accuracy')\n    ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n    \n    if history_fine:\n        offset = len(history.history['accuracy'])\n        ax1.plot(range(offset, offset + len(history_fine.history['accuracy'])), \n                 history_fine.history['accuracy'], label='Train Accuracy (Fine-tune)')\n        ax1.plot(range(offset, offset + len(history_fine.history['val_accuracy'])), \n                 history_fine.history['val_accuracy'], label='Val Accuracy (Fine-tune)')\n    \n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Accuracy')\n    ax1.set_title('Model Accuracy')\n    ax1.legend()\n    ax1.grid(True)\n    \n    # Plot loss\n    ax2.plot(history.history['loss'], label='Train Loss')\n    ax2.plot(history.history['val_loss'], label='Val Loss')\n    \n    if history_fine:\n        offset = len(history.history['loss'])\n        ax2.plot(range(offset, offset + len(history_fine.history['loss'])), \n                 history_fine.history['loss'], label='Train Loss (Fine-tune)')\n        ax2.plot(range(offset, offset + len(history_fine.history['val_loss'])), \n                 history_fine.history['val_loss'], label='Val Loss (Fine-tune)')\n    \n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Loss')\n    ax2.set_title('Model Loss')\n    ax2.legend()\n    ax2.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('model/training_history.png', dpi=150)\n    print(\"\\nTraining history plot saved to model/training_history.png\")\n\n\ndef create_demo_model():\n    \"\"\"\n    Create and save a demo model with random weights\n    This is just for demonstration - train with real data for actual use\n    \"\"\"\n    print(\"Creating demo model...\")\n    model = create_model()\n    \n    # Save demo model\n    os.makedirs('model', exist_ok=True)\n    model.save('model/currency_detector.h5')\n    print(\"Demo model saved to model/currency_detector.h5\")\n    print(\"\\nWARNING: This is an untrained model with random weights!\")\n    print(\"For real use, train the model with actual currency dataset.\")\n\n\nif __name__ == '__main__':\n    # Check if dataset directories exist\n    train_dir = 'dataset/train'\n    val_dir = 'dataset/val'\n    \n    if os.path.exists(train_dir) and os.path.exists(val_dir):\n        # Train with actual dataset\n        print(\"Dataset found. Starting training...\")\n        print(f\"Train directory: {train_dir}\")\n        print(f\"Validation directory: {val_dir}\")\n        print(\"\\nExpected structure:\")\n        print(\"  dataset/train/fake/\")\n        print(\"  dataset/train/genuine/\")\n        print(\"  dataset/val/fake/\")\n        print(\"  dataset/val/genuine/\\n\")\n        \n        model, history = train_model(train_dir, val_dir, epochs=20)\n    else:\n        # Create demo model\n        print(\"No dataset found.\")\n        print(f\"Expected directories: {train_dir}, {val_dir}\")\n        print(\"\\nCreating demo model for testing purposes...\")\n        create_demo_model()\n","size_bytes":5991},"train_indian_currency_model.py":{"content":"\"\"\"\nTraining Script for Indian Currency Counterfeit Detection\nTrains model on 50, 200, and 500 Rupee notes (genuine vs fake)\n\"\"\"\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\ndef create_model(input_shape=(224, 224, 3), num_classes=2):\n    \"\"\"\n    Create MobileNetV2-based model for currency detection\n    \n    Args:\n        input_shape: Input image shape\n        num_classes: Number of classes (2: genuine/fake)\n    \n    Returns:\n        Compiled model\n    \"\"\"\n    print(\"Creating MobileNetV2-based model...\")\n    \n    # Load pre-trained MobileNetV2 (without top layer)\n    base_model = MobileNetV2(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet'\n    )\n    \n    # Freeze base model\n    base_model.trainable = False\n    \n    # Build model\n    inputs = keras.Input(shape=input_shape)\n    \n    # Preprocessing for MobileNetV2\n    x = keras.applications.mobilenet_v2.preprocess_input(inputs)\n    \n    # Base model\n    x = base_model(x, training=False)\n    \n    # Classification head\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(128, activation='relu')(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = keras.Model(inputs, outputs)\n    \n    # Compile\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    print(\"âœ“ Model created successfully\")\n    return model\n\ndef fine_tune_model(model, num_layers_to_unfreeze=20):\n    \"\"\"\n    Fine-tune the model by unfreezing some layers\n    \n    Args:\n        model: Trained model\n        num_layers_to_unfreeze: Number of layers to unfreeze from the end\n    \n    Returns:\n        Model ready for fine-tuning\n    \"\"\"\n    print(f\"\\nFine-tuning: Unfreezing last {num_layers_to_unfreeze} layers...\")\n    \n    # Find the MobileNetV2 base model by searching for it\n    base_model = None\n    for layer in model.layers:\n        if 'mobilenet' in layer.name.lower():\n            base_model = layer\n            break\n    \n    if base_model is None:\n        print(\"Warning: Could not find MobileNetV2 base model. Skipping fine-tuning.\")\n        return model\n    \n    # Unfreeze the last layers\n    base_model.trainable = True\n    for layer in base_model.layers[:-num_layers_to_unfreeze]:\n        layer.trainable = False\n    \n    # Recompile with lower learning rate\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    print(\"âœ“ Model prepared for fine-tuning\")\n    return model\n\ndef create_data_generators(train_dir, val_dir, batch_size=16, target_size=(224, 224)):\n    \"\"\"\n    Create data generators with augmentation\n    \n    Args:\n        train_dir: Training data directory\n        val_dir: Validation data directory\n        batch_size: Batch size\n        target_size: Target image size\n    \n    Returns:\n        Training and validation generators\n    \"\"\"\n    print(\"Creating data generators with augmentation...\")\n    \n    # Training data augmentation\n    train_datagen = ImageDataGenerator(\n        rotation_range=10,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        shear_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    \n    # Validation data (no augmentation)\n    val_datagen = ImageDataGenerator()\n    \n    # Create generators\n    train_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=target_size,\n        batch_size=batch_size,\n        class_mode='categorical',\n        shuffle=True\n    )\n    \n    val_generator = val_datagen.flow_from_directory(\n        val_dir,\n        target_size=target_size,\n        batch_size=batch_size,\n        class_mode='categorical',\n        shuffle=False\n    )\n    \n    print(f\"âœ“ Data generators created\")\n    print(f\"  Classes: {train_generator.class_indices}\")\n    return train_generator, val_generator\n\ndef plot_training_history(history, history_fine=None, save_path='model/training_history.png'):\n    \"\"\"Plot and save training history\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Plot accuracy\n    ax1.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n    ax1.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n    \n    if history_fine:\n        offset = len(history.history['accuracy'])\n        ax1.plot(range(offset, offset + len(history_fine.history['accuracy'])), \n                 history_fine.history['accuracy'], label='Train Acc (Fine-tune)', linewidth=2)\n        ax1.plot(range(offset, offset + len(history_fine.history['val_accuracy'])), \n                 history_fine.history['val_accuracy'], label='Val Acc (Fine-tune)', linewidth=2)\n    \n    ax1.set_xlabel('Epoch', fontsize=12)\n    ax1.set_ylabel('Accuracy', fontsize=12)\n    ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n    ax1.legend(fontsize=10)\n    ax1.grid(True, alpha=0.3)\n    \n    # Plot loss\n    ax2.plot(history.history['loss'], label='Train Loss', linewidth=2)\n    ax2.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n    \n    if history_fine:\n        offset = len(history.history['loss'])\n        ax2.plot(range(offset, offset + len(history_fine.history['loss'])), \n                 history_fine.history['loss'], label='Train Loss (Fine-tune)', linewidth=2)\n        ax2.plot(range(offset, offset + len(history_fine.history['val_loss'])), \n                 history_fine.history['val_loss'], label='Val Loss (Fine-tune)', linewidth=2)\n    \n    ax2.set_xlabel('Epoch', fontsize=12)\n    ax2.set_ylabel('Loss', fontsize=12)\n    ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n    ax2.legend(fontsize=10)\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    print(f\"\\nâœ“ Training history plot saved to {save_path}\")\n\ndef train_model(train_dir='indian_currency_dataset/train', \n                val_dir='indian_currency_dataset/val',\n                epochs=15,\n                fine_tune_epochs=10,\n                batch_size=16):\n    \"\"\"\n    Train the Indian currency detection model\n    \n    Args:\n        train_dir: Training directory\n        val_dir: Validation directory\n        epochs: Number of initial training epochs\n        fine_tune_epochs: Number of fine-tuning epochs\n        batch_size: Batch size\n    \n    Returns:\n        Trained model and history\n    \"\"\"\n    print(\"=\"*70)\n    print(\"TRAINING INDIAN CURRENCY COUNTERFEIT DETECTION MODEL\")\n    print(\"=\"*70)\n    print(f\"Train directory: {train_dir}\")\n    print(f\"Validation directory: {val_dir}\")\n    print(f\"Initial epochs: {epochs}\")\n    print(f\"Fine-tune epochs: {fine_tune_epochs}\")\n    print(f\"Batch size: {batch_size}\")\n    print(\"=\"*70)\n    \n    # Create model\n    model = create_model()\n    print(f\"\\nModel architecture:\")\n    model.summary()\n    \n    # Create data generators\n    train_generator, val_generator = create_data_generators(\n        train_dir, val_dir, batch_size\n    )\n    \n    print(f\"\\nDataset:\")\n    print(f\"  Training samples: {train_generator.samples}\")\n    print(f\"  Validation samples: {val_generator.samples}\")\n    print(f\"  Classes: {train_generator.class_indices}\")\n    \n    # Create model directory\n    os.makedirs('model', exist_ok=True)\n    \n    # Callbacks\n    callbacks = [\n        keras.callbacks.ModelCheckpoint(\n            'model/indian_currency_detector_best.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            verbose=1\n        ),\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            min_lr=1e-7,\n            verbose=1\n        )\n    ]\n    \n    # Phase 1: Initial training\n    print(\"\\n\" + \"=\"*70)\n    print(\"PHASE 1: Initial Training (Frozen Base Model)\")\n    print(\"=\"*70)\n    \n    history = model.fit(\n        train_generator,\n        epochs=epochs,\n        validation_data=val_generator,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Phase 2: Fine-tuning\n    print(\"\\n\" + \"=\"*70)\n    print(\"PHASE 2: Fine-Tuning (Unfreezing Last Layers)\")\n    print(\"=\"*70)\n    \n    model = fine_tune_model(model)\n    \n    history_fine = model.fit(\n        train_generator,\n        epochs=fine_tune_epochs,\n        validation_data=val_generator,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Save final model\n    model.save('model/indian_currency_detector.h5')\n    print(\"\\nâœ“ Final model saved to model/indian_currency_detector.h5\")\n    \n    # Also save to CounterfeitGuard directory\n    model.save('CounterfeitGuard/model/currency_detector.h5')\n    print(\"âœ“ Model copied to CounterfeitGuard/model/currency_detector.h5\")\n    \n    # Evaluate\n    print(\"\\n\" + \"=\"*70)\n    print(\"FINAL EVALUATION\")\n    print(\"=\"*70)\n    \n    val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\n    print(f\"Validation Accuracy: {val_accuracy*100:.2f}%\")\n    print(f\"Validation Loss: {val_loss:.4f}\")\n    \n    # Plot training history\n    plot_training_history(history, history_fine)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"TRAINING COMPLETE!\")\n    print(\"=\"*70)\n    print(\"Model saved to:\")\n    print(\"  â€¢ model/indian_currency_detector.h5\")\n    print(\"  â€¢ CounterfeitGuard/model/currency_detector.h5\")\n    print(\"Training history plot:\")\n    print(\"  â€¢ model/training_history.png\")\n    print(\"=\"*70)\n    \n    return model, history, history_fine\n\nif __name__ == '__main__':\n    # Check if dataset exists\n    train_dir = 'indian_currency_dataset/train'\n    val_dir = 'indian_currency_dataset/val'\n    \n    if not os.path.exists(train_dir) or not os.path.exists(val_dir):\n        print(\"=\"*70)\n        print(\"ERROR: Dataset not found!\")\n        print(\"=\"*70)\n        print(f\"Expected directories:\")\n        print(f\"  {train_dir}\")\n        print(f\"  {val_dir}\")\n        print(\"\\nPlease run first:\")\n        print(\"  python create_indian_currency_dataset.py\")\n        print(\"=\"*70)\n    else:\n        # Train the model\n        model, history, history_fine = train_model(\n            train_dir=train_dir,\n            val_dir=val_dir,\n            epochs=15,\n            fine_tune_epochs=10,\n            batch_size=16\n        )\n","size_bytes":10835},"train_rupee_500_model.py":{"content":"\"\"\"\nTrain Currency Detection Model Specifically for Indian â‚¹500 Notes\nThis script trains from scratch with proper preprocessing and augmentation\n\"\"\"\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Configuration\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 8  # Small batch size due to limited data\nEPOCHS_PHASE1 = 30  # Initial training with frozen base\nEPOCHS_PHASE2 = 20  # Fine-tuning phase\nTRAIN_DIR = 'dataset_500_only/train'\nVAL_DIR = 'dataset_500_only/val'\nMODEL_SAVE_PATH = 'CounterfeitGuard/model/currency_detector.h5'\nCHECKPOINT_PATH = 'CounterfeitGuard/model/rupee_500_checkpoint.h5'\n\nprint(\"=\"*70)\nprint(\"TRAINING INDIAN â‚¹500 NOTE COUNTERFEIT DETECTION MODEL\")\nprint(\"=\"*70)\n\n# Verify dataset structure\nprint(f\"\\nDataset directories:\")\nprint(f\"  Training: {TRAIN_DIR}\")\nprint(f\"  Validation: {VAL_DIR}\")\n\n# Count images\ntrain_fake = len(os.listdir(os.path.join(TRAIN_DIR, 'fake')))\ntrain_genuine = len(os.listdir(os.path.join(TRAIN_DIR, 'genuine')))\nval_fake = len(os.listdir(os.path.join(VAL_DIR, 'fake')))\nval_genuine = len(os.listdir(os.path.join(VAL_DIR, 'genuine')))\n\nprint(f\"\\nDataset statistics:\")\nprint(f\"  Training: {train_fake} fake + {train_genuine} genuine = {train_fake + train_genuine} total\")\nprint(f\"  Validation: {val_fake} fake + {val_genuine} genuine = {val_fake + val_genuine} total\")\nprint(f\"  Class balance: {(train_fake/(train_fake+train_genuine)*100):.1f}% fake, {(train_genuine/(train_fake+train_genuine)*100):.1f}% genuine\")\n\n# Create model save directory\nos.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n\ndef create_model(input_shape=(224, 224, 3), num_classes=2):\n    \"\"\"\n    Create CNN model using MobileNetV2 transfer learning\n    Optimized for â‚¹500 note detection\n    \"\"\"\n    print(\"\\nBuilding model architecture...\")\n    \n    # Load pre-trained MobileNetV2\n    base_model = MobileNetV2(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet'\n    )\n    \n    # Freeze base model for initial training\n    base_model.trainable = False\n    print(f\"Base model loaded: {len(base_model.layers)} layers (frozen)\")\n    \n    # Build custom classification head\n    inputs = keras.Input(shape=input_shape)\n    x = base_model(inputs, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    \n    # Dense layers with dropout for regularization\n    x = layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)\n    x = layers.Dropout(0.3)(x)\n    \n    # Output layer - 2 classes (fake, genuine)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = keras.Model(inputs, outputs, name='rupee_500_detector')\n    \n    # Compile model\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n    )\n    \n    print(f\"Model created: {model.count_params():,} total parameters\")\n    return model, base_model\n\n\ndef create_data_generators():\n    \"\"\"\n    Create data generators with augmentation\n    CRITICAL: Use same preprocessing as prediction\n    \"\"\"\n    print(\"\\nSetting up data generators...\")\n    \n    # Training data with aggressive augmentation (small dataset)\n    train_datagen = ImageDataGenerator(\n        preprocessing_function=keras.applications.mobilenet_v2.preprocess_input,\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        vertical_flip=False,  # Currency notes shouldn't be upside down\n        brightness_range=[0.8, 1.2],\n        fill_mode='nearest'\n    )\n    \n    # Validation data - only preprocessing, no augmentation\n    val_datagen = ImageDataGenerator(\n        preprocessing_function=keras.applications.mobilenet_v2.preprocess_input\n    )\n    \n    # Create generators\n    train_generator = train_datagen.flow_from_directory(\n        TRAIN_DIR,\n        target_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=True,\n        seed=42\n    )\n    \n    val_generator = val_datagen.flow_from_directory(\n        VAL_DIR,\n        target_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=False\n    )\n    \n    print(f\"Training generator: {train_generator.samples} samples\")\n    print(f\"Validation generator: {val_generator.samples} samples\")\n    print(f\"Class indices: {train_generator.class_indices}\")\n    \n    return train_generator, val_generator\n\n\ndef train_phase_1(model, train_gen, val_gen):\n    \"\"\"\n    Phase 1: Train with frozen base model\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"PHASE 1: Training with frozen MobileNetV2 base\")\n    print(\"=\"*70)\n    \n    # Callbacks\n    checkpoint = ModelCheckpoint(\n        CHECKPOINT_PATH,\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n    \n    early_stop = EarlyStopping(\n        monitor='val_accuracy',\n        patience=10,\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    reduce_lr = ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=5,\n        min_lr=1e-7,\n        verbose=1\n    )\n    \n    # Train\n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=EPOCHS_PHASE1,\n        callbacks=[checkpoint, early_stop, reduce_lr],\n        verbose=1\n    )\n    \n    return history\n\n\ndef train_phase_2(model, base_model, train_gen, val_gen):\n    \"\"\"\n    Phase 2: Fine-tune top layers of base model\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"PHASE 2: Fine-tuning top layers of MobileNetV2\")\n    print(\"=\"*70)\n    \n    # Unfreeze top layers\n    base_model.trainable = True\n    unfroze_from = len(base_model.layers) - 30  # Unfreeze last 30 layers\n    \n    for layer in base_model.layers[:unfroze_from]:\n        layer.trainable = False\n    \n    trainable_layers = sum([1 for layer in base_model.layers if layer.trainable])\n    print(f\"Unfroze {trainable_layers} layers from base model\")\n    \n    # Recompile with lower learning rate\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n    )\n    \n    # Callbacks\n    checkpoint = ModelCheckpoint(\n        CHECKPOINT_PATH,\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n    \n    early_stop = EarlyStopping(\n        monitor='val_accuracy',\n        patience=8,\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    reduce_lr = ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=4,\n        min_lr=1e-8,\n        verbose=1\n    )\n    \n    # Fine-tune\n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=EPOCHS_PHASE2,\n        callbacks=[checkpoint, early_stop, reduce_lr],\n        verbose=1\n    )\n    \n    return history\n\n\ndef plot_training_history(history1, history2):\n    \"\"\"Plot training metrics\"\"\"\n    print(\"\\nGenerating training plots...\")\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Combine histories\n    metrics = {\n        'accuracy': [],\n        'val_accuracy': [],\n        'loss': [],\n        'val_loss': []\n    }\n    \n    for key in metrics.keys():\n        if key in history1.history:\n            metrics[key].extend(history1.history[key])\n        if key in history2.history:\n            metrics[key].extend(history2.history[key])\n    \n    epochs = range(1, len(metrics['accuracy']) + 1)\n    \n    # Accuracy\n    axes[0, 0].plot(epochs, metrics['accuracy'], 'b-', label='Training', linewidth=2)\n    axes[0, 0].plot(epochs, metrics['val_accuracy'], 'r-', label='Validation', linewidth=2)\n    axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Accuracy')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Loss\n    axes[0, 1].plot(epochs, metrics['loss'], 'b-', label='Training', linewidth=2)\n    axes[0, 1].plot(epochs, metrics['val_loss'], 'r-', label='Validation', linewidth=2)\n    axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('Loss')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Phase separation line\n    phase1_end = len(history1.history.get('accuracy', []))\n    for ax in axes.flat:\n        if phase1_end > 0:\n            ax.axvline(x=phase1_end, color='green', linestyle='--', linewidth=2, label='Fine-tuning starts', alpha=0.7)\n    \n    plt.tight_layout()\n    plot_path = 'CounterfeitGuard/model/rupee_500_training.png'\n    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n    print(f\"Training plot saved to: {plot_path}\")\n    plt.close()\n\n\ndef evaluate_model(model, val_gen):\n    \"\"\"Evaluate final model performance\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"FINAL MODEL EVALUATION\")\n    print(\"=\"*70)\n    \n    results = model.evaluate(val_gen, verbose=1)\n    \n    print(f\"\\nValidation Results:\")\n    for metric, value in zip(model.metrics_names, results):\n        print(f\"  {metric}: {value:.4f}\")\n    \n    return results\n\n\ndef main():\n    \"\"\"Main training pipeline\"\"\"\n    print(\"\\nStarting training pipeline...\")\n    \n    # Create data generators\n    train_gen, val_gen = create_data_generators()\n    \n    # Create model\n    model, base_model = create_model()\n    \n    # Phase 1: Train with frozen base\n    history1 = train_phase_1(model, train_gen, val_gen)\n    \n    # Phase 2: Fine-tune\n    history2 = train_phase_2(model, base_model, train_gen, val_gen)\n    \n    # Plot training history\n    plot_training_history(history1, history2)\n    \n    # Final evaluation\n    evaluate_model(model, val_gen)\n    \n    # Save final model\n    print(f\"\\nSaving final model to: {MODEL_SAVE_PATH}\")\n    model.save(MODEL_SAVE_PATH)\n    file_size = os.path.getsize(MODEL_SAVE_PATH) / (1024 * 1024)\n    print(f\"Model saved successfully! Size: {file_size:.2f} MB\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"TRAINING COMPLETE!\")\n    print(\"=\"*70)\n    print(f\"\\nModel ready for â‚¹500 note counterfeit detection\")\n    print(f\"Model location: {MODEL_SAVE_PATH}\")\n    print(\"\\nNext steps:\")\n    print(\"1. Test the model with sample â‚¹500 notes\")\n    print(\"2. Verify predictions are not cached between images\")\n    print(\"3. Deploy to production environment\")\n\n\nif __name__ == '__main__':\n    main()\n","size_bytes":10964},"main.py":{"content":"from CounterfeitGuard.app import app, db\n\n# Initialize database tables on import\nwith app.app_context():\n    db.create_all()\n    print(\"Database initialized successfully\")\n\nif __name__ == \"__main__\":\n    # Run the Flask app (only for local development)\n    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n","size_bytes":304},"CounterfeitGuard/README.md":{"content":"# Fake Currency Detector\n\nAI-powered web application that detects whether a currency note is genuine or fake using deep learning.\n\n## Features\n\n- **CNN-based Detection**: Uses MobileNetV2 transfer learning for binary classification\n- **High Accuracy**: Designed to achieve >96% accuracy with proper training data\n- **Grad-CAM Visualization**: Highlights suspicious regions on the currency note\n- **Confidence Scores**: Shows probability percentages for genuine vs fake\n- **Simple Web Interface**: Easy-to-use upload and analyze interface\n- **Real-time Analysis**: Quick predictions with visual feedback\n\n## Tech Stack\n\n- **Backend**: Flask 3.0\n- **ML Framework**: TensorFlow 2.15 with Keras\n- **Model**: MobileNetV2 (transfer learning)\n- **Image Processing**: OpenCV, Pillow\n- **Visualization**: Matplotlib, Grad-CAM\n- **Frontend**: HTML, CSS, JavaScript\n\n## Project Structure\n\n```\n.\nâ”œâ”€â”€ app.py                  # Flask API with /predict endpoint\nâ”œâ”€â”€ model.py                # CNN model architecture and utilities\nâ”œâ”€â”€ train_model.py          # Training script for the model\nâ”œâ”€â”€ requirements.txt        # Python dependencies\nâ”œâ”€â”€ templates/\nâ”‚   â””â”€â”€ index.html         # Web interface\nâ”œâ”€â”€ model/\nâ”‚   â””â”€â”€ currency_detector.h5  # Trained model (created after training)\nâ””â”€â”€ uploads/               # Temporary storage for uploaded images\n```\n\n## Installation\n\nThe required packages are already installed in this Replit environment:\n\n- tensorflow==2.15.0\n- flask==3.0.0\n- opencv-python==4.8.1.78\n- numpy==1.24.3\n- pillow==10.1.0\n- matplotlib==3.8.2\n- werkzeug==3.0.1\n\n## Usage\n\n### Running the Application\n\nThe Flask app is already configured to run automatically. Simply:\n\n1. Click the \"Run\" button or access the webview\n2. Upload a currency note image (PNG, JPG, or JPEG)\n3. Click \"Analyze Currency\"\n4. View results with confidence scores and Grad-CAM visualization\n\n### API Endpoints\n\n#### `GET /`\nReturns the main web interface\n\n#### `POST /predict`\nAccepts image file and returns prediction results\n\n**Request:**\n- Method: POST\n- Content-Type: multipart/form-data\n- Body: `file` (image file)\n\n**Response:**\n```json\n{\n  \"prediction\": \"Genuine\" or \"Fake\",\n  \"confidence\": 95.34,\n  \"is_genuine\": true,\n  \"probabilities\": {\n    \"fake\": 4.66,\n    \"genuine\": 95.34\n  },\n  \"gradcam_image\": \"/gradcam/gradcam_filename.png\"\n}\n```\n\n#### `GET /gradcam/<filename>`\nReturns the Grad-CAM visualization image\n\n## Training Your Own Model\n\nThe current model is a demo with random weights. To train with real data:\n\n1. **Prepare Dataset**: Organize images into this structure:\n   ```\n   dataset/\n   â”œâ”€â”€ train/\n   â”‚   â”œâ”€â”€ fake/       # Fake currency images\n   â”‚   â””â”€â”€ genuine/    # Genuine currency images\n   â””â”€â”€ val/\n       â”œâ”€â”€ fake/       # Validation fake images\n       â””â”€â”€ genuine/    # Validation genuine images\n   ```\n\n2. **Run Training**:\n   ```bash\n   python train_model.py\n   ```\n\n3. **Training Features**:\n   - Transfer learning with MobileNetV2\n   - Data augmentation (rotation, zoom, brightness, flip)\n   - Early stopping to prevent overfitting\n   - Learning rate reduction on plateau\n   - Model checkpointing (saves best model)\n   - Fine-tuning of top layers\n\n4. **Training Outputs**:\n   - `model/currency_detector.h5` - Final trained model\n   - `model/currency_detector_best.h5` - Best model during training\n   - `model/training_history.png` - Training metrics visualization\n\n## Model Architecture\n\n- **Base Model**: MobileNetV2 (pretrained on ImageNet)\n- **Custom Layers**:\n  - Global Average Pooling\n  - Dense(256) + ReLU + Dropout(0.5)\n  - Dense(128) + ReLU + Dropout(0.3)\n  - Dense(2) + Softmax (output)\n\n- **Input Shape**: 224x224x3 RGB images\n- **Output**: 2 classes (Fake, Genuine)\n\n## Data Augmentation\n\nThe training pipeline includes:\n- Rotation (Â±20 degrees)\n- Width/Height shift (20%)\n- Shear transformation (20%)\n- Zoom (20%)\n- Horizontal flip\n- Brightness adjustment (80-120%)\n\n## Grad-CAM Visualization\n\nGradient-weighted Class Activation Mapping (Grad-CAM) shows:\n- Which parts of the currency note the model focused on\n- Red/yellow regions indicate high importance\n- Blue regions indicate low importance\n- Helps identify suspicious features on fake notes\n\n## Important Notes\n\n1. **Demo Model**: The current model has random weights and won't give accurate predictions\n2. **Training Data**: You need real currency images (genuine and fake) to train properly\n3. **Accuracy Target**: With proper dataset, the model should achieve >96% accuracy\n4. **Legal Disclaimer**: This is for educational purposes only. Consult local laws regarding currency image handling\n\n## How It Works\n\n1. **Upload**: User uploads currency note image\n2. **Preprocessing**: Image is resized to 224x224 and preprocessed for MobileNet\n3. **Prediction**: Model classifies as genuine or fake with confidence score\n4. **Grad-CAM**: Generates heatmap showing important regions\n5. **Display**: Results shown with overlaid visualization\n\n## Performance Tips\n\n- Use clear, well-lit images\n- Ensure the entire note is visible\n- Avoid blurry or distorted images\n- Higher resolution images work better\n\n## Future Enhancements\n\n- Denomination detection\n- Currency type identification\n- Batch processing\n- REST API documentation\n- Model performance metrics dashboard\n- Database logging of predictions\n\n## License\n\nEducational project - use responsibly and in accordance with local laws.\n","size_bytes":5452},"replit.md":{"content":"# Fake Currency Detector\n\n## Overview\n\nAn AI-powered web application that detects whether a currency note is genuine or fake using deep learning. The system uses transfer learning with MobileNetV2 architecture and provides visual explanations through Grad-CAM (Gradient-weighted Class Activation Mapping) heatmaps that highlight suspicious regions on currency notes.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## Recent Changes\n\n### November 2, 2025 (Afternoon Update)\n- **Fixed Prediction Caching Issue**: Resolved critical bug where first image's prediction was being reused\n  - Added unique timestamp-based filenames to prevent file conflicts\n  - Implemented `tf.keras.backend.clear_session()` to clear TensorFlow session state before each prediction\n  - Added comprehensive debugging output showing image path, array stats, and raw predictions for each request\n  - Each prediction now creates a completely fresh image array and preprocessing pipeline\n  \n- **Retrained Model for â‚¹500 Notes Only**:\n  - Created dedicated training script: `train_rupee_500_model.py`\n  - Dataset: 28 training images (16 fake + 12 genuine), 7 validation images (4 fake + 3 genuine)\n  - Two-phase training approach:\n    - Phase 1: 30 epochs with frozen MobileNetV2 base\n    - Phase 2: 20 epochs fine-tuning last 30 layers\n  - **Final Results**: 100% training accuracy, 100% validation accuracy\n  - Model size: 24.79 MB\n  - Heavy data augmentation: rotation, shift, shear, zoom, brightness, horizontal flip\n  \n- **UI Updates for â‚¹500 Specificity**:\n  - Updated page title to \"Indian â‚¹500 Note Detector\"\n  - Changed upload prompt to specifically mention â‚¹500 notes\n  - Result messages now show \"GENUINE â‚¹500 NOTE\" or \"FAKE â‚¹500 NOTE\"\n  - Updated info banner to highlight model is trained specifically for â‚¹500 notes\n  - Improved \"Upload Another\" button with clear â‚¹500 note reference\n  \n- **Code Quality Improvements**:\n  - Added detailed comments explaining fresh array creation in preprocessing\n  - Extensive logging for debugging prediction caching issues\n  - Improved model info response to indicate â‚¹500 note specialization\n\n### November 2, 2025 (Morning)\n- **Kaggle Dataset Integration & Model Training**: Successfully integrated Kaggle API for dataset management\n  - Installed Kaggle package and configured API authentication using environment secrets\n  - Downloaded and organized Indian currency counterfeit detection dataset\n  - Final dataset: 72 training images (32 fake + 40 genuine), 18 validation images (8 fake + 10 genuine)\n  \n- **Two-Phase Transfer Learning Training**:\n  - **Phase 1 (Initial Training)**: Frozen MobileNetV2 base model, 15 epochs\n    - Training accuracy: 100%\n    - Validation accuracy: 94.44%\n    - Early stopping triggered after 12 epochs\n  - **Phase 2 (Fine-Tuning)**: Unfroze last 20 layers, 10 epochs, lower learning rate (0.0001)\n    - Training accuracy: 100%\n    - **Final validation accuracy: 100%**\n    - Improvement: +5.56%\n  - Model saved to: `CounterfeitGuard/model/currency_detector.h5` (14MB)\n  \n- **Training Infrastructure**:\n  - Scripts: `download_kaggle_dataset.py`, `train_indian_currency_model.py`, `complete_training.py`\n  - Callbacks: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n  - Data augmentation: rotation, shift, shear, zoom, horizontal flip\n  - Fixed fine-tuning bug by properly identifying MobileNetV2 layer\n  \n- **Model Performance Notes** (from architect review):\n  - Small dataset (90 total images) means 100% accuracy should be interpreted cautiously\n  - Likely overfitting on validation set due to limited samples\n  - Recommendations for future: expand dataset with real captures, implement cross-validation, enhance augmentation\n  \n- **Previous Training**: Initial model with stock images + synthetic fakes\n  - Evaluation script: `evaluate_model.py` with confusion matrix visualization\n\n### November 1, 2025\n- **Upload Storage**: Modified upload directory from project folder to temporary directory (`/tmp/currency_uploads`)\n  - Prevents project folder pollution\n  - Uses secure filename handling to prevent directory traversal attacks\n\n## System Architecture\n\n### Frontend Architecture\n- **Technology**: HTML templates with Jinja2 templating engine\n- **Interface Components**: \n  - Main detection page with drag-and-drop file upload\n  - Testing dashboard for model debugging and batch testing\n  - Authentication pages (login/register)\n  - Base template system for consistent navigation and layout\n- **Styling**: Custom CSS with modern design patterns (gradients, shadows, responsive cards)\n- **Client-side Logic**: JavaScript for file handling, preview generation, and asynchronous API communication\n\n### Backend Architecture\n- **Framework**: Flask 3.0 web application\n- **Design Pattern**: MVC-style separation with templates, routes, and model logic\n- **API Endpoints**:\n  - `/predict` - Image classification endpoint\n  - Authentication routes (login, register, logout)\n  - Testing dashboard route\n- **File Handling**: Werkzeug secure filename processing with 16MB upload limit\n- **Session Management**: Flask-Login for user authentication with session-based authentication\n\n### Machine Learning Architecture\n- **Base Model**: MobileNetV2 pre-trained on ImageNet (transfer learning approach)\n- **Model Structure**:\n  - Frozen MobileNetV2 base for feature extraction\n  - Global average pooling layer\n  - Custom classification head with dense layers (256 â†’ 128 neurons)\n  - Dropout regularization (0.5 and 0.3) to prevent overfitting\n  - Softmax output for binary classification (genuine/fake)\n- **Input**: 224x224x3 RGB images\n- **Training Strategy**: Two-phase training (frozen base, then fine-tuning)\n- **Visualization**: Grad-CAM implementation for explainable AI - highlights image regions influencing predictions\n\n### Data Storage\n- **Database**: SQLite with SQLAlchemy ORM\n- **Schema**: User model with email and password hash fields\n- **Security**: Werkzeug password hashing (generate_password_hash/check_password_hash)\n- **File Storage**: Local filesystem for uploaded images and trained models\n- **Model Persistence**: Keras H5 format for saved models\n\n### Authentication & Authorization\n- **Library**: Flask-Login for session management\n- **User Model**: SQLAlchemy-based User class implementing UserMixin\n- **Password Security**: Hashed passwords using Werkzeug security utilities\n- **Protected Routes**: `@login_required` decorator for restricted endpoints\n- **Session**: Server-side session with configurable secret key\n\n## External Dependencies\n\n### Core ML Framework\n- **TensorFlow 2.15**: Deep learning framework for model training and inference\n- **Keras**: High-level neural networks API (part of TensorFlow)\n- **MobileNetV2 Weights**: Pre-trained ImageNet weights downloaded from Keras applications\n\n### Image Processing\n- **OpenCV 4.8.1**: Computer vision library for image preprocessing\n- **Pillow 10.1.0**: Python Imaging Library for image I/O and manipulation\n- **NumPy 1.24.3**: Numerical computing for array operations\n\n### Web Framework\n- **Flask 3.0.0**: Lightweight WSGI web application framework\n- **Flask-SQLAlchemy**: ORM integration for database operations\n- **Flask-Login**: User session management\n- **Werkzeug 3.0.1**: WSGI utilities and security helpers\n\n### Visualization\n- **Matplotlib 3.8.2**: Plotting library for Grad-CAM heatmap generation\n\n### Training Infrastructure\n- **Data Augmentation**: ImageDataGenerator for training data preprocessing and augmentation (rotation, zoom, shift, flip)\n- **Synthetic Data Generation**: Custom script for creating demonstration currency images when real datasets unavailable\n- **Callbacks**: ModelCheckpoint for saving best models, EarlyStopping for training optimization\n\n### Deployment Environment\n- **Platform**: Replit with Python 3.11 runtime\n- **Port Configuration**: Flask runs on port 5000 with webview\n- **Scaling**: Autoscale deployment mode\n- **Environment Variables**: SECRET_KEY for session security","size_bytes":8005},"scripts/download_kaggle_dataset.py":{"content":"\"\"\"\nDownload and prepare Kaggle dataset for Indian currency counterfeit detection\n\"\"\"\nimport os\nimport subprocess\nimport zipfile\nimport shutil\nfrom pathlib import Path\n\ndef setup_kaggle_credentials():\n    \"\"\"Set up Kaggle credentials from environment variables\"\"\"\n    kaggle_dir = Path.home() / '.kaggle'\n    kaggle_dir.mkdir(exist_ok=True)\n    \n    kaggle_json = {\n        \"username\": os.environ.get('KAGGLE_USERNAME'),\n        \"key\": os.environ.get('KAGGLE_KEY')\n    }\n    \n    kaggle_json_path = kaggle_dir / 'kaggle.json'\n    import json\n    with open(kaggle_json_path, 'w') as f:\n        json.dump(kaggle_json, f)\n    \n    # Set proper permissions\n    os.chmod(kaggle_json_path, 0o600)\n    print(f\"Kaggle credentials set up at {kaggle_json_path}\")\n\ndef download_dataset():\n    \"\"\"Download the fake currency dataset from Kaggle\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Downloading Indian Fake Currency Dataset from Kaggle...\")\n    print(\"=\"*60 + \"\\n\")\n    \n    dataset_name = \"lekhansaathvik/fake-currency-dataset\"\n    download_path = \"./kaggle_dataset\"\n    \n    # Create download directory\n    os.makedirs(download_path, exist_ok=True)\n    \n    # Download using Kaggle API\n    try:\n        subprocess.run([\n            \"kaggle\", \"datasets\", \"download\", \n            \"-d\", dataset_name,\n            \"-p\", download_path,\n            \"--unzip\"\n        ], check=True)\n        print(f\"\\nâœ“ Dataset downloaded successfully to {download_path}\")\n        return download_path\n    except subprocess.CalledProcessError as e:\n        print(f\"Error downloading dataset: {e}\")\n        return None\n\ndef organize_dataset(download_path):\n    \"\"\"Organize the dataset into train/val structure\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Organizing dataset for training...\")\n    print(\"=\"*60 + \"\\n\")\n    \n    # Check what files are in the download directory\n    print(f\"Contents of {download_path}:\")\n    for root, dirs, files in os.walk(download_path):\n        level = root.replace(download_path, '').count(os.sep)\n        indent = ' ' * 2 * level\n        print(f'{indent}{os.path.basename(root)}/')\n        subindent = ' ' * 2 * (level + 1)\n        for file in files[:5]:  # Show first 5 files\n            print(f'{subindent}{file}')\n        if len(files) > 5:\n            print(f'{subindent}... and {len(files) - 5} more files')\n    \n    # Look for train and validation directories\n    train_dir = Path(\"dataset/train\")\n    val_dir = Path(\"dataset/val\")\n    \n    # Create directories\n    for split in ['train', 'val']:\n        for category in ['fake', 'genuine']:\n            os.makedirs(f\"dataset/{split}/{category}\", exist_ok=True)\n    \n    # Find and organize the files\n    download_path = Path(download_path)\n    \n    # Try to find the dataset structure\n    possible_paths = [\n        download_path / \"train\",\n        download_path / \"Train\",\n        download_path / \"training\",\n        download_path,\n    ]\n    \n    # Look for image files and organize them\n    train_fake = list(download_path.rglob(\"*fake*.png\")) + list(download_path.rglob(\"*fake*.jpg\")) + list(download_path.rglob(\"*Fake*.png\"))\n    train_genuine = list(download_path.rglob(\"*genuine*.png\")) + list(download_path.rglob(\"*genuine*.jpg\")) + list(download_path.rglob(\"*real*.png\"))\n    \n    # If we can't find organized structure, try the existing indian_currency_dataset\n    if os.path.exists(\"indian_currency_dataset/train\"):\n        print(\"\\nFound existing indian_currency_dataset! Using that instead...\")\n        \n        # Copy from indian_currency_dataset to dataset\n        if os.path.exists(\"indian_currency_dataset/train/fake\"):\n            src = \"indian_currency_dataset/train\"\n            dst = \"dataset/train\"\n            if os.path.exists(dst):\n                shutil.rmtree(dst)\n            shutil.copytree(src, dst)\n            print(f\"âœ“ Copied training data from indian_currency_dataset\")\n        \n        if os.path.exists(\"indian_currency_dataset/val/fake\"):\n            src = \"indian_currency_dataset/val\"\n            dst = \"dataset/val\"\n            if os.path.exists(dst):\n                shutil.rmtree(dst)\n            shutil.copytree(src, dst)\n            print(f\"âœ“ Copied validation data from indian_currency_dataset\")\n        \n        return True\n    \n    # If no structured data found, try to organize the downloaded files\n    print(f\"\\nFound {len(train_fake)} fake images and {len(train_genuine)} genuine images\")\n    \n    if len(train_fake) == 0 and len(train_genuine) == 0:\n        print(\"\\nWarning: Could not find fake/genuine images in Kaggle dataset\")\n        print(\"Please check the dataset structure manually\")\n        return False\n    \n    # Split 80-20 for train/val\n    from sklearn.model_selection import train_test_split\n    \n    if train_fake:\n        fake_train, fake_val = train_test_split(train_fake, test_size=0.2, random_state=42)\n        \n        for img_path in fake_train:\n            shutil.copy(img_path, f\"dataset/train/fake/{img_path.name}\")\n        \n        for img_path in fake_val:\n            shutil.copy(img_path, f\"dataset/val/fake/{img_path.name}\")\n        \n        print(f\"âœ“ Organized {len(fake_train)} fake training images\")\n        print(f\"âœ“ Organized {len(fake_val)} fake validation images\")\n    \n    if train_genuine:\n        genuine_train, genuine_val = train_test_split(train_genuine, test_size=0.2, random_state=42)\n        \n        for img_path in genuine_train:\n            shutil.copy(img_path, f\"dataset/train/genuine/{img_path.name}\")\n        \n        for img_path in genuine_val:\n            shutil.copy(img_path, f\"dataset/val/genuine/{img_path.name}\")\n        \n        print(f\"âœ“ Organized {len(genuine_train)} genuine training images\")\n        print(f\"âœ“ Organized {len(genuine_val)} genuine validation images\")\n    \n    return True\n\ndef print_dataset_summary():\n    \"\"\"Print summary of organized dataset\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Dataset Summary\")\n    print(\"=\"*60 + \"\\n\")\n    \n    for split in ['train', 'val']:\n        for category in ['fake', 'genuine']:\n            path = f\"dataset/{split}/{category}\"\n            if os.path.exists(path):\n                count = len([f for f in os.listdir(path) if f.endswith(('.png', '.jpg', '.jpeg'))])\n                print(f\"{split.capitalize():12} {category.capitalize():10}: {count:4} images\")\n            else:\n                print(f\"{split.capitalize():12} {category.capitalize():10}: Directory not found\")\n    \n    print(\"\\nâœ“ Dataset is ready for training!\")\n    print(\"Run: uv run python train_indian_currency_model.py\")\n\nif __name__ == '__main__':\n    # Set up Kaggle credentials\n    setup_kaggle_credentials()\n    \n    # Download dataset\n    download_path = download_dataset()\n    \n    if download_path:\n        # Organize dataset\n        success = organize_dataset(download_path)\n        \n        if success:\n            # Print summary\n            print_dataset_summary()\n        else:\n            print(\"\\nNote: Using existing indian_currency_dataset if available\")\n            print_dataset_summary()\n    else:\n        print(\"\\nFailed to download dataset. Checking for existing dataset...\")\n        organize_dataset(\"./\")\n        print_dataset_summary()\n","size_bytes":7164},"CounterfeitGuard/create_dataset.py":{"content":"\"\"\"\nCreate a synthetic currency image dataset for training\nGenerates realistic-looking currency note images for demonstration\n\"\"\"\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont, ImageFilter\nimport os\nimport random\nfrom pathlib import Path\n\ndef create_currency_image(genuine=True, seed=None):\n    \"\"\"\n    Generate a synthetic currency note image\n    \n    Args:\n        genuine: If True, create genuine note, else create fake note\n        seed: Random seed for reproducibility\n    \n    Returns:\n        PIL Image\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    # Create base image (224x224 for MobileNet)\n    width, height = 448, 224\n    \n    # Base colors\n    if genuine:\n        # Genuine notes have more consistent, vibrant colors\n        base_color = random.choice([\n            (34, 139, 34),   # Forest green\n            (25, 25, 112),   # Midnight blue\n            (139, 69, 19),   # Saddle brown\n            (128, 0, 128),   # Purple\n        ])\n        texture_intensity = 0.05\n        watermark_alpha = 180\n    else:\n        # Fake notes have slightly off colors and poor quality\n        base_color = random.choice([\n            (50, 150, 50),   # Slightly off green\n            (40, 40, 120),   # Slightly off blue\n            (150, 80, 30),   # Slightly off brown\n            (140, 10, 140),  # Slightly off purple\n        ])\n        texture_intensity = 0.15\n        watermark_alpha = 100\n    \n    # Create base with noise\n    img = Image.new('RGB', (width, height), base_color)\n    pixels = np.array(img)\n    \n    # Add texture noise\n    noise = np.random.normal(0, texture_intensity * 255, pixels.shape)\n    pixels = np.clip(pixels + noise, 0, 255).astype(np.uint8)\n    img = Image.fromarray(pixels)\n    \n    draw = ImageDraw.Draw(img, 'RGBA')\n    \n    # Add geometric patterns\n    num_lines = 15 if genuine else random.randint(8, 12)\n    for i in range(num_lines):\n        x = random.randint(0, width)\n        color = tuple(np.clip(np.array(base_color) + np.random.randint(-30, 30, 3), 0, 255).tolist())\n        draw.line([(x, 0), (x, height)], fill=color, width=1)\n    \n    # Add circles (security features)\n    num_circles = random.randint(3, 6) if genuine else random.randint(1, 3)\n    for _ in range(num_circles):\n        x, y = random.randint(20, width-20), random.randint(20, height-20)\n        r = random.randint(10, 30)\n        circle_color = tuple(np.clip(np.array(base_color) + 50, 0, 255).tolist())\n        draw.ellipse([x-r, y-r, x+r, y+r], outline=circle_color, width=2)\n    \n    # Add denomination number\n    denomination = random.choice([10, 20, 50, 100])\n    try:\n        font_size = 60 if genuine else random.randint(50, 65)\n        \n        # Add number in corners\n        for pos in [(30, 30), (width-80, 30), (30, height-90), (width-80, height-90)]:\n            text_color = (255, 255, 255, watermark_alpha)\n            shadow_offset = 2\n            # Shadow\n            draw.text((pos[0]+shadow_offset, pos[1]+shadow_offset), str(denomination), \n                     fill=(0, 0, 0, watermark_alpha//2))\n            # Main text\n            draw.text(pos, str(denomination), fill=text_color)\n    except:\n        pass\n    \n    # Add watermark patterns\n    if genuine:\n        # Genuine notes have clear, consistent watermarks\n        for i in range(5):\n            x = width // 6 * (i + 1)\n            draw.ellipse([x-15, height//2-15, x+15, height//2+15], \n                        outline=(255, 255, 255, 150), width=2)\n    else:\n        # Fake notes have irregular watermarks\n        for i in range(random.randint(2, 4)):\n            x = random.randint(50, width-50)\n            y = random.randint(50, height-50)\n            draw.ellipse([x-10, y-10, x+10, y+10], \n                        outline=(255, 255, 255, 80), width=1)\n    \n    # Add some blur to fake notes\n    if not genuine:\n        img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.3, 0.8)))\n    \n    # Resize to 224x224\n    img = img.resize((224, 224), Image.Resampling.LANCZOS)\n    \n    # Add final quality variations\n    if not genuine:\n        # Reduce quality slightly for fake notes\n        enhancer = np.random.uniform(0.85, 0.95)\n        pixels = np.array(img).astype(float)\n        pixels = np.clip(pixels * enhancer, 0, 255).astype(np.uint8)\n        img = Image.fromarray(pixels)\n    \n    return img\n\ndef create_dataset(num_images_per_class=150, train_split=0.8):\n    \"\"\"\n    Create a complete dataset with train/val splits\n    \n    Args:\n        num_images_per_class: Number of images per class (genuine/fake)\n        train_split: Proportion for training set\n    \"\"\"\n    print(\"Creating synthetic currency dataset...\")\n    print(f\"Generating {num_images_per_class} genuine and {num_images_per_class} fake notes\")\n    \n    # Create directory structure\n    dataset_dir = Path('dataset')\n    for split in ['train', 'val']:\n        for category in ['fake', 'genuine']:\n            (dataset_dir / split / category).mkdir(parents=True, exist_ok=True)\n    \n    # Determine split\n    num_train = int(num_images_per_class * train_split)\n    num_val = num_images_per_class - num_train\n    \n    # Generate images\n    for category, is_genuine in [('genuine', True), ('fake', False)]:\n        print(f\"\\nGenerating {category} notes...\")\n        \n        # Training images\n        for i in range(num_train):\n            img = create_currency_image(genuine=is_genuine, seed=i)\n            img.save(dataset_dir / 'train' / category / f'{category}_{i:04d}.jpg', quality=95)\n            if (i + 1) % 50 == 0:\n                print(f\"  Generated {i+1}/{num_train} training images\")\n        \n        # Validation images\n        for i in range(num_val):\n            img = create_currency_image(genuine=is_genuine, seed=num_train + i)\n            img.save(dataset_dir / 'val' / category / f'{category}_{i:04d}.jpg', quality=95)\n        \n        print(f\"  Completed {category}: {num_train} train, {num_val} val\")\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*60)\n    print(\"Dataset created successfully!\")\n    print(\"=\"*60)\n    print(f\"Train - Genuine: {num_train} images\")\n    print(f\"Train - Fake: {num_train} images\")\n    print(f\"Val - Genuine: {num_val} images\")\n    print(f\"Val - Fake: {num_val} images\")\n    print(f\"Total: {num_images_per_class * 2} images\")\n    print(\"\\nDataset structure:\")\n    print(\"  dataset/train/genuine/\")\n    print(\"  dataset/train/fake/\")\n    print(\"  dataset/val/genuine/\")\n    print(\"  dataset/val/fake/\")\n\nif __name__ == '__main__':\n    # Create dataset with 300 images (150 genuine + 150 fake)\n    create_dataset(num_images_per_class=150, train_split=0.8)\n    print(\"\\nYou can now run: python train_model.py\")\n","size_bytes":6709},"CounterfeitGuard/replit.md":{"content":"# Fake Currency Detector Web App\n\n## Overview\nA CNN-based web application that detects whether a currency note is genuine or fake using deep learning. The app uses transfer learning with MobileNet architecture and provides Grad-CAM visualizations to highlight suspicious regions on the currency notes.\n\n## Project Architecture\n- **Backend**: Flask API with TensorFlow/Keras for ML inference\n- **Model**: MobileNet-based CNN with custom classification layers\n- **Visualization**: Grad-CAM heatmap overlays on input images\n- **Frontend**: Simple HTML/CSS interface for image uploads\n\n## Recent Changes\n- **Testing Dashboard Added** (Oct 31, 2025)\n  - Created comprehensive testing page at `/testing`\n  - Model information display (status, layers, input/output shapes)\n  - Batch image testing capability\n  - Model architecture inspector showing all layers\n  - Quick test results with confidence scores\n  - Link from main app to testing dashboard\n- **Grad-CAM Layer Fix** (Oct 31, 2025)\n  - Fixed Grad-CAM function to auto-detect the correct convolutional layer\n  - Updated make_gradcam_heatmap to access layers within MobileNetV2 base model\n  - App now handles both trained and demo models correctly\n- **Replit Environment Setup** (Oct 31, 2025)\n  - Imported from GitHub and configured for Replit\n  - Python 3.11 module installed\n  - All dependencies installed (TensorFlow 2.15, Flask 3.0, OpenCV, etc.)\n  - Workflow configured to run Flask app on port 5000 with webview\n  - Deployment configuration set up (autoscale mode)\n  - Created model and uploads directories\n  - MobileNetV2 pretrained weights downloaded successfully\n  - Demo model created (no trained model yet - needs dataset)\n  - Web interface verified and working\n  - Root .gitignore added for Python artifacts\n- **Previous Development** (Before import)\n  - Model architecture created with MobileNetV2 transfer learning\n  - Training scripts completed (train_model.py)\n  - Grad-CAM visualization implemented\n  - Web interface designed\n\n## Tech Stack\n- Python 3.11\n- TensorFlow 2.15 (CNN model with MobileNet transfer learning)\n- Flask 3.0 (REST API)\n- OpenCV (image preprocessing)\n- NumPy, Pillow, Matplotlib (data processing and visualization)\n\n## Features\n1. Binary classification: genuine vs fake currency\n2. Transfer learning using MobileNet\n3. Data augmentation for improved accuracy\n4. Grad-CAM visualization for explainability\n5. Confidence score display\n6. Simple web interface for image upload\n\n## Target\n- Model accuracy: >96%\n","size_bytes":2495},"create_indian_currency_dataset.py":{"content":"\n\"\"\"\nCreate Indian Currency Dataset from Test Images\nUses actual fake and genuine currency images for training\n\"\"\"\nimport os\nimport shutil\nfrom pathlib import Path\n\ndef create_dataset():\n    \"\"\"Create train/val split from Test Images directory\"\"\"\n    \n    print(\"=\"*70)\n    print(\"CREATING INDIAN CURRENCY DATASET WITH ACTUAL IMAGES\")\n    print(\"=\"*70)\n    \n    # Source directories\n    genuine_source = \"Test Images/genuine\"\n    fake_source = \"Test Images/fake\"\n    \n    # Destination directories\n    dataset_root = \"indian_currency_dataset\"\n    train_dir = os.path.join(dataset_root, \"train\")\n    val_dir = os.path.join(dataset_root, \"val\")\n    \n    # Create directory structure\n    for split in ['train', 'val']:\n        for class_name in ['fake', 'genuine']:\n            os.makedirs(os.path.join(dataset_root, split, class_name), exist_ok=True)\n    \n    # Get all images\n    genuine_images = sorted([f for f in os.listdir(genuine_source) if f.endswith(('.jpg', '.png', '.jpeg'))])\n    fake_images = sorted([f for f in os.listdir(fake_source) if f.endswith(('.jpg', '.png', '.jpeg'))])\n    \n    print(f\"\\nFound {len(genuine_images)} genuine currency images\")\n    print(f\"Found {len(fake_images)} fake currency images\")\n    \n    # Split: 80% train, 20% validation\n    genuine_split = int(len(genuine_images) * 0.8)\n    fake_split = int(len(fake_images) * 0.8)\n    \n    genuine_train = genuine_images[:genuine_split]\n    genuine_val = genuine_images[genuine_split:]\n    \n    fake_train = fake_images[:fake_split]\n    fake_val = fake_images[fake_split:]\n    \n    print(f\"\\nTrain set: {len(genuine_train)} genuine + {len(fake_train)} fake\")\n    print(f\"Val set: {len(genuine_val)} genuine + {len(fake_val)} fake\")\n    \n    print(\"\\nCopying images to dataset...\")\n    \n    # Copy training images\n    for img in genuine_train:\n        src = os.path.join(genuine_source, img)\n        dst = os.path.join(train_dir, 'genuine', img)\n        shutil.copy2(src, dst)\n    \n    for img in fake_train:\n        src = os.path.join(fake_source, img)\n        dst = os.path.join(train_dir, 'fake', img)\n        shutil.copy2(src, dst)\n    \n    # Copy validation images\n    for img in genuine_val:\n        src = os.path.join(genuine_source, img)\n        dst = os.path.join(val_dir, 'genuine', img)\n        shutil.copy2(src, dst)\n    \n    for img in fake_val:\n        src = os.path.join(fake_source, img)\n        dst = os.path.join(val_dir, 'fake', img)\n        shutil.copy2(src, dst)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"DATASET CREATED SUCCESSFULLY!\")\n    print(\"=\"*70)\n    \n    print(f\"\\nDataset structure:\")\n    print(f\"  {train_dir}/fake/ - {len(fake_train)} images\")\n    print(f\"  {train_dir}/genuine/ - {len(genuine_train)} images\")\n    print(f\"  {val_dir}/fake/ - {len(fake_val)} images\")\n    print(f\"  {val_dir}/genuine/ - {len(genuine_val)} images\")\n    print(\"=\"*70)\n    \n    print(\"\\nNext step: Run 'python train_indian_currency_model.py' to train the model\")\n    print(\"=\"*70)\n\nif __name__ == '__main__':\n    create_dataset()\n","size_bytes":3022},"CounterfeitGuard/create_user.py":{"content":"#!/usr/bin/env python\n\"\"\"Script to create a user account\"\"\"\nfrom app import app, db, User\n\ndef create_user(username, password):\n    with app.app_context():\n        # Check if user already exists\n        existing_user = User.query.filter_by(username=username).first()\n        if existing_user:\n            print(f\"User '{username}' already exists!\")\n            return False\n        \n        # Create new user\n        user = User(username=username)\n        user.set_password(password)\n        db.session.add(user)\n        db.session.commit()\n        print(f\"User '{username}' created successfully!\")\n        return True\n\nif __name__ == '__main__':\n    # Create a demo user\n    username = \"admin\"\n    password = \"admin123\"\n    \n    create_user(username, password)\n    print(f\"\\nLogin credentials:\")\n    print(f\"Username: {username}\")\n    print(f\"Password: {password}\")\n","size_bytes":867},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"email-validator>=2.3.0\",\n    \"flask==3.0.0\",\n    \"flask-login>=0.6.3\",\n    \"flask-sqlalchemy>=3.1.1\",\n    \"gunicorn>=23.0.0\",\n    \"kaggle>=1.7.4.5\",\n    \"kagglehub>=0.3.13\",\n    \"matplotlib==3.8.2\",\n    \"numpy==1.24.3\",\n    \"opencv-python==4.8.1.78\",\n    \"pillow==10.1.0\",\n    \"psycopg2-binary>=2.9.11\",\n    \"requests>=2.32.5\",\n    \"scikit-learn>=1.7.2\",\n    \"scipy>=1.15.3\",\n    \"seaborn>=0.13.2\",\n    \"tensorflow==2.15.0\",\n    \"werkzeug==3.0.1\",\n]\n","size_bytes":597},"train_500_only.py":{"content":"\n\"\"\"\nTrain model specifically for 500 Rupee notes detection\n\"\"\"\nimport os\nimport shutil\nfrom pathlib import Path\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\n\ndef create_500_dataset():\n    \"\"\"Create dataset with only 500 rupee notes\"\"\"\n    print(\"=\"*70)\n    print(\"CREATING 500 RUPEE DATASET\")\n    print(\"=\"*70)\n    \n    # Source directories\n    genuine_source = \"Test Images/genuine\"\n    fake_source = \"Test Images/fake\"\n    \n    # Destination\n    dataset_root = \"dataset_500_only\"\n    train_dir = os.path.join(dataset_root, \"train\")\n    val_dir = os.path.join(dataset_root, \"val\")\n    \n    # Create structure\n    for split in ['train', 'val']:\n        for class_name in ['fake', 'genuine']:\n            os.makedirs(os.path.join(dataset_root, split, class_name), exist_ok=True)\n    \n    # Get 500 rupee images (filter by filename containing \"500\")\n    genuine_images = [f for f in os.listdir(genuine_source) \n                     if f.endswith(('.jpg', '.png', '.jpeg')) and '500' in f]\n    # For fake, we'll use all since they're counterfeit training samples\n    fake_images = [f for f in os.listdir(fake_source) \n                  if f.endswith(('.jpg', '.png', '.jpeg'))]\n    \n    print(f\"\\nFound {len(genuine_images)} genuine 500 rupee images\")\n    print(f\"Found {len(fake_images)} fake currency images\")\n    \n    # Split: 80% train, 20% val\n    genuine_split = int(len(genuine_images) * 0.8)\n    fake_split = int(len(fake_images) * 0.8)\n    \n    genuine_train = genuine_images[:genuine_split]\n    genuine_val = genuine_images[genuine_split:]\n    fake_train = fake_images[:fake_split]\n    fake_val = fake_images[fake_split:]\n    \n    print(f\"\\nTrain: {len(genuine_train)} genuine + {len(fake_train)} fake\")\n    print(f\"Val: {len(genuine_val)} genuine + {len(fake_val)} fake\")\n    \n    # Copy images\n    for img in genuine_train:\n        shutil.copy2(os.path.join(genuine_source, img), \n                    os.path.join(train_dir, 'genuine', img))\n    \n    for img in fake_train:\n        shutil.copy2(os.path.join(fake_source, img), \n                    os.path.join(train_dir, 'fake', img))\n    \n    for img in genuine_val:\n        shutil.copy2(os.path.join(genuine_source, img), \n                    os.path.join(val_dir, 'genuine', img))\n    \n    for img in fake_val:\n        shutil.copy2(os.path.join(fake_source, img), \n                    os.path.join(val_dir, 'fake', img))\n    \n    print(\"\\nâœ“ Dataset created successfully!\")\n    return dataset_root\n\ndef create_model(input_shape=(224, 224, 3), num_classes=2):\n    \"\"\"Create MobileNetV2-based model\"\"\"\n    print(\"\\nCreating model...\")\n    \n    base_model = MobileNetV2(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet'\n    )\n    base_model.trainable = False\n    \n    inputs = keras.Input(shape=input_shape)\n    x = keras.applications.mobilenet_v2.preprocess_input(inputs)\n    x = base_model(x, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(128, activation='relu')(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    print(\"âœ“ Model created\")\n    return model\n\ndef fine_tune_model(model, num_layers=20):\n    \"\"\"Fine-tune by unfreezing layers\"\"\"\n    print(f\"\\nFine-tuning: unfreezing last {num_layers} layers...\")\n    \n    base_model = None\n    for layer in model.layers:\n        if 'mobilenet' in layer.name.lower():\n            base_model = layer\n            break\n    \n    if base_model:\n        base_model.trainable = True\n        for layer in base_model.layers[:-num_layers]:\n            layer.trainable = False\n        \n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n    \n    print(\"âœ“ Model ready for fine-tuning\")\n    return model\n\ndef train_500_model():\n    \"\"\"Train model for 500 rupee notes\"\"\"\n    print(\"=\"*70)\n    print(\"TRAINING MODEL FOR 500 RUPEE NOTES\")\n    print(\"=\"*70)\n    \n    # Create dataset\n    dataset_root = create_500_dataset()\n    train_dir = os.path.join(dataset_root, \"train\")\n    val_dir = os.path.join(dataset_root, \"val\")\n    \n    # Create model\n    model = create_model()\n    \n    # Data generators\n    train_datagen = ImageDataGenerator(\n        rotation_range=10,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        shear_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    \n    val_datagen = ImageDataGenerator()\n    \n    train_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=(224, 224),\n        batch_size=16,\n        class_mode='categorical',\n        shuffle=True\n    )\n    \n    val_generator = val_datagen.flow_from_directory(\n        val_dir,\n        target_size=(224, 224),\n        batch_size=16,\n        class_mode='categorical',\n        shuffle=False\n    )\n    \n    print(f\"\\nDataset info:\")\n    print(f\"  Training samples: {train_generator.samples}\")\n    print(f\"  Validation samples: {val_generator.samples}\")\n    print(f\"  Classes: {train_generator.class_indices}\")\n    \n    # Create model directory\n    os.makedirs('model', exist_ok=True)\n    os.makedirs('CounterfeitGuard/model', exist_ok=True)\n    \n    # Callbacks\n    callbacks = [\n        keras.callbacks.ModelCheckpoint(\n            'model/currency_detector_500_best.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            verbose=1\n        ),\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            min_lr=1e-7,\n            verbose=1\n        )\n    ]\n    \n    # Phase 1: Initial training\n    print(\"\\n\" + \"=\"*70)\n    print(\"PHASE 1: Initial Training\")\n    print(\"=\"*70)\n    \n    history = model.fit(\n        train_generator,\n        epochs=15,\n        validation_data=val_generator,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Phase 2: Fine-tuning\n    print(\"\\n\" + \"=\"*70)\n    print(\"PHASE 2: Fine-Tuning\")\n    print(\"=\"*70)\n    \n    model = fine_tune_model(model)\n    \n    history_fine = model.fit(\n        train_generator,\n        epochs=10,\n        validation_data=val_generator,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Save final model\n    model.save('model/currency_detector.h5')\n    model.save('CounterfeitGuard/model/currency_detector.h5')\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"TRAINING COMPLETE!\")\n    print(\"=\"*70)\n    \n    # Evaluate\n    val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\n    print(f\"\\nFinal Validation Accuracy: {val_accuracy*100:.2f}%\")\n    print(f\"Final Validation Loss: {val_loss:.4f}\")\n    \n    print(\"\\nModel saved to:\")\n    print(\"  â€¢ model/currency_detector.h5\")\n    print(\"  â€¢ CounterfeitGuard/model/currency_detector.h5\")\n    print(\"=\"*70)\n    \n    # Plot history\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    \n    ax1.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n    ax1.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n    if history_fine:\n        offset = len(history.history['accuracy'])\n        ax1.plot(range(offset, offset + len(history_fine.history['accuracy'])), \n                 history_fine.history['accuracy'], label='Train Acc (Fine-tune)', linewidth=2)\n        ax1.plot(range(offset, offset + len(history_fine.history['val_accuracy'])), \n                 history_fine.history['val_accuracy'], label='Val Acc (Fine-tune)', linewidth=2)\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Accuracy')\n    ax1.set_title('Model Accuracy - 500 Rupee Training')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    ax2.plot(history.history['loss'], label='Train Loss', linewidth=2)\n    ax2.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n    if history_fine:\n        offset = len(history.history['loss'])\n        ax2.plot(range(offset, offset + len(history_fine.history['loss'])), \n                 history_fine.history['loss'], label='Train Loss (Fine-tune)', linewidth=2)\n        ax2.plot(range(offset, offset + len(history_fine.history['val_loss'])), \n                 history_fine.history['val_loss'], label='Val Loss (Fine-tune)', linewidth=2)\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Loss')\n    ax2.set_title('Model Loss - 500 Rupee Training')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('model/training_history_500.png', dpi=150)\n    print(f\"\\nâœ“ Training plot saved to model/training_history_500.png\")\n\nif __name__ == '__main__':\n    train_500_model()\n","size_bytes":9297},"evaluate_model.py":{"content":"\"\"\"\nEvaluate the trained Indian Currency Counterfeit Detection Model\n\"\"\"\nimport os\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\ndef load_trained_model(model_path='model/indian_currency_detector_best.h5'):\n    \"\"\"Load the trained model\"\"\"\n    print(f\"Loading model from {model_path}...\")\n    model = keras.models.load_model(model_path)\n    print(\"âœ“ Model loaded successfully\")\n    return model\n\ndef evaluate_on_validation_set(model, val_dir='indian_currency_dataset/val'):\n    \"\"\"Evaluate model on validation set\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"EVALUATING ON VALIDATION SET\")\n    print(\"=\"*70)\n    \n    # Create data generator (no augmentation for validation)\n    val_datagen = ImageDataGenerator()\n    val_generator = val_datagen.flow_from_directory(\n        val_dir,\n        target_size=(224, 224),\n        batch_size=1,\n        class_mode='categorical',\n        shuffle=False\n    )\n    \n    print(f\"Validation samples: {val_generator.samples}\")\n    print(f\"Classes: {val_generator.class_indices}\")\n    \n    # Evaluate\n    print(\"\\nEvaluating...\")\n    loss, accuracy = model.evaluate(val_generator, verbose=0)\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"VALIDATION RESULTS\")\n    print(f\"{'='*70}\")\n    print(f\"Accuracy: {accuracy*100:.2f}%\")\n    print(f\"Loss: {loss:.4f}\")\n    \n    # Get predictions\n    print(\"\\nGenerating predictions...\")\n    predictions = model.predict(val_generator, verbose=0)\n    predicted_classes = np.argmax(predictions, axis=1)\n    true_classes = val_generator.classes\n    class_labels = list(val_generator.class_indices.keys())\n    \n    # Confusion matrix\n    cm = confusion_matrix(true_classes, predicted_classes)\n    \n    print(f\"\\nConfusion Matrix:\")\n    print(f\"{'='*70}\")\n    print(f\"                 Predicted\")\n    print(f\"               Fake  Genuine\")\n    print(f\"Actual  Fake    {cm[0][0]:3d}     {cm[0][1]:3d}\")\n    print(f\"      Genuine  {cm[1][0]:3d}     {cm[1][1]:3d}\")\n    \n    # Classification report\n    print(f\"\\n{'='*70}\")\n    print(\"DETAILED CLASSIFICATION REPORT\")\n    print(f\"{'='*70}\")\n    print(classification_report(true_classes, predicted_classes, \n                                target_names=class_labels, digits=4))\n    \n    # Sample predictions\n    print(f\"\\n{'='*70}\")\n    print(\"SAMPLE PREDICTIONS\")\n    print(f\"{'='*70}\")\n    filenames = val_generator.filenames\n    for i in range(min(12, len(filenames))):\n        true_label = class_labels[true_classes[i]]\n        pred_label = class_labels[predicted_classes[i]]\n        confidence = predictions[i][predicted_classes[i]] * 100\n        status = \"âœ“\" if true_label == pred_label else \"âœ—\"\n        print(f\"{status} {filenames[i]:50s} | True: {true_label:8s} | Pred: {pred_label:8s} ({confidence:5.1f}%)\")\n    \n    return accuracy, predictions, true_classes, predicted_classes\n\ndef create_confusion_matrix_plot(true_classes, predicted_classes, class_labels):\n    \"\"\"Create and save confusion matrix plot\"\"\"\n    cm = confusion_matrix(true_classes, predicted_classes)\n    \n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_labels, yticklabels=class_labels,\n                cbar_kws={'label': 'Count'})\n    plt.title('Confusion Matrix - Indian Currency Detection', fontsize=14, fontweight='bold')\n    plt.ylabel('True Label', fontsize=12)\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.tight_layout()\n    \n    save_path = 'model/confusion_matrix.png'\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    print(f\"\\nâœ“ Confusion matrix saved to {save_path}\")\n\ndef main():\n    \"\"\"Main evaluation\"\"\"\n    print(\"=\"*70)\n    print(\"INDIAN CURRENCY COUNTERFEIT DETECTION - MODEL EVALUATION\")\n    print(\"=\"*70)\n    \n    # Load model\n    model = load_trained_model()\n    \n    # Evaluate on validation set\n    accuracy, predictions, true_classes, predicted_classes = evaluate_on_validation_set(model)\n    \n    # Create visualizations\n    class_labels = ['fake', 'genuine']\n    create_confusion_matrix_plot(true_classes, predicted_classes, class_labels)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"EVALUATION COMPLETE!\")\n    print(\"=\"*70)\n    print(f\"Final Validation Accuracy: {accuracy*100:.2f}%\")\n    print(\"\\nOutput files:\")\n    print(\"  â€¢ model/confusion_matrix.png\")\n    print(\"=\"*70)\n\nif __name__ == '__main__':\n    main()\n","size_bytes":4538},"add_user.py":{"content":"#!/usr/bin/env python\n\"\"\"Script to create a user account for the Currency Detector app\"\"\"\nimport sys\nsys.path.insert(0, '.')\n\nfrom flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom werkzeug.security import generate_password_hash\nimport os\n\n# Create minimal Flask app just for database operations\napp = Flask(__name__)\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///CounterfeitGuard/instance/currency_detector.db'\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n\ndb = SQLAlchemy(app)\n\nclass User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True, nullable=False)\n    password_hash = db.Column(db.String(200), nullable=False)\n\ndef create_user(username, password):\n    with app.app_context():\n        # Check if user already exists\n        existing_user = User.query.filter_by(username=username).first()\n        if existing_user:\n            print(f\"User '{username}' already exists!\")\n            return False\n        \n        # Create new user\n        user = User(username=username)\n        user.password_hash = generate_password_hash(password)\n        db.session.add(user)\n        db.session.commit()\n        print(f\"âœ“ User '{username}' created successfully!\")\n        return True\n\nif __name__ == '__main__':\n    # Create admin user\n    username = \"admin\"\n    password = \"admin123\"\n    \n    create_user(username, password)\n    print(f\"\\nYour login credentials:\")\n    print(f\"  Username: {username}\")\n    print(f\"  Password: {password}\")\n    print(\"\\nYou can now login to the Currency Detector app!\")\n","size_bytes":1587}},"version":2}